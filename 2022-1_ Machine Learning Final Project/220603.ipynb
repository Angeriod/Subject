{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be60f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb9b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./Train.csv')\n",
    "df_test=pd.read_csv('./Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fb253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0caa949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X991</th>\n",
       "      <th>X992</th>\n",
       "      <th>X993</th>\n",
       "      <th>X994</th>\n",
       "      <th>X995</th>\n",
       "      <th>X996</th>\n",
       "      <th>X997</th>\n",
       "      <th>X998</th>\n",
       "      <th>X999</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.977776</td>\n",
       "      <td>-1.517843</td>\n",
       "      <td>0.912956</td>\n",
       "      <td>-0.912691</td>\n",
       "      <td>2.474028</td>\n",
       "      <td>-1.520725</td>\n",
       "      <td>0.789179</td>\n",
       "      <td>2.132584</td>\n",
       "      <td>1.238520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>2.637154</td>\n",
       "      <td>1.453993</td>\n",
       "      <td>2.007524</td>\n",
       "      <td>-1.048487</td>\n",
       "      <td>-0.657207</td>\n",
       "      <td>1.162462</td>\n",
       "      <td>-0.694344</td>\n",
       "      <td>-0.746024</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.043404</td>\n",
       "      <td>-1.070653</td>\n",
       "      <td>1.220211</td>\n",
       "      <td>1.201270</td>\n",
       "      <td>2.743644</td>\n",
       "      <td>-1.659054</td>\n",
       "      <td>0.701626</td>\n",
       "      <td>4.002729</td>\n",
       "      <td>1.234633</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762113</td>\n",
       "      <td>0.394766</td>\n",
       "      <td>2.394662</td>\n",
       "      <td>3.641124</td>\n",
       "      <td>-1.235631</td>\n",
       "      <td>-0.305115</td>\n",
       "      <td>1.162605</td>\n",
       "      <td>-0.628928</td>\n",
       "      <td>-0.715540</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.805436</td>\n",
       "      <td>-1.585968</td>\n",
       "      <td>1.254656</td>\n",
       "      <td>1.242043</td>\n",
       "      <td>2.766510</td>\n",
       "      <td>-1.596180</td>\n",
       "      <td>0.719818</td>\n",
       "      <td>1.885536</td>\n",
       "      <td>1.329080</td>\n",
       "      <td>...</td>\n",
       "      <td>1.698334</td>\n",
       "      <td>2.192528</td>\n",
       "      <td>2.596114</td>\n",
       "      <td>2.167297</td>\n",
       "      <td>-1.176496</td>\n",
       "      <td>-0.633323</td>\n",
       "      <td>0.996117</td>\n",
       "      <td>-0.695098</td>\n",
       "      <td>-0.745621</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.148511</td>\n",
       "      <td>-1.130018</td>\n",
       "      <td>1.230766</td>\n",
       "      <td>1.276613</td>\n",
       "      <td>2.753486</td>\n",
       "      <td>-1.573832</td>\n",
       "      <td>0.793888</td>\n",
       "      <td>2.182489</td>\n",
       "      <td>1.336875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756445</td>\n",
       "      <td>2.552250</td>\n",
       "      <td>2.611069</td>\n",
       "      <td>2.055999</td>\n",
       "      <td>-0.986814</td>\n",
       "      <td>-0.332215</td>\n",
       "      <td>1.129808</td>\n",
       "      <td>-0.691850</td>\n",
       "      <td>-0.707566</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.155129</td>\n",
       "      <td>-1.500327</td>\n",
       "      <td>1.143260</td>\n",
       "      <td>1.058527</td>\n",
       "      <td>2.757607</td>\n",
       "      <td>-1.572982</td>\n",
       "      <td>0.665892</td>\n",
       "      <td>2.138818</td>\n",
       "      <td>1.485586</td>\n",
       "      <td>...</td>\n",
       "      <td>1.707510</td>\n",
       "      <td>2.463747</td>\n",
       "      <td>2.387666</td>\n",
       "      <td>1.957450</td>\n",
       "      <td>-1.170196</td>\n",
       "      <td>-0.705918</td>\n",
       "      <td>1.116841</td>\n",
       "      <td>-0.694471</td>\n",
       "      <td>-0.772044</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index        X0        X1        X2        X3        X4        X5  \\\n",
       "0      1  0.977776 -1.517843  0.912956 -0.912691  2.474028 -1.520725   \n",
       "1      2  1.043404 -1.070653  1.220211  1.201270  2.743644 -1.659054   \n",
       "2      3  0.805436 -1.585968  1.254656  1.242043  2.766510 -1.596180   \n",
       "3      4  3.148511 -1.130018  1.230766  1.276613  2.753486 -1.573832   \n",
       "4      5  1.155129 -1.500327  1.143260  1.058527  2.757607 -1.572982   \n",
       "\n",
       "         X6        X7        X8  ...      X991      X992      X993      X994  \\\n",
       "0  0.789179  2.132584  1.238520  ...  1.951992  2.637154  1.453993  2.007524   \n",
       "1  0.701626  4.002729  1.234633  ...  1.762113  0.394766  2.394662  3.641124   \n",
       "2  0.719818  1.885536  1.329080  ...  1.698334  2.192528  2.596114  2.167297   \n",
       "3  0.793888  2.182489  1.336875  ...  1.756445  2.552250  2.611069  2.055999   \n",
       "4  0.665892  2.138818  1.485586  ...  1.707510  2.463747  2.387666  1.957450   \n",
       "\n",
       "       X995      X996      X997      X998      X999    target  \n",
       "0 -1.048487 -0.657207  1.162462 -0.694344 -0.746024  negative  \n",
       "1 -1.235631 -0.305115  1.162605 -0.628928 -0.715540  negative  \n",
       "2 -1.176496 -0.633323  0.996117 -0.695098 -0.745621  negative  \n",
       "3 -0.986814 -0.332215  1.129808 -0.691850 -0.707566  negative  \n",
       "4 -1.170196 -0.705918  1.116841 -0.694471 -0.772044  negative  \n",
       "\n",
       "[5 rows x 1002 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2588b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=df['target'].apply(lambda x:0 if x=='negative' else 1 )\n",
    "df=df.drop(['Index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2f7edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X990</th>\n",
       "      <th>X991</th>\n",
       "      <th>X992</th>\n",
       "      <th>X993</th>\n",
       "      <th>X994</th>\n",
       "      <th>X995</th>\n",
       "      <th>X996</th>\n",
       "      <th>X997</th>\n",
       "      <th>X998</th>\n",
       "      <th>X999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.641059</td>\n",
       "      <td>-1.556056</td>\n",
       "      <td>-1.173289</td>\n",
       "      <td>1.239458</td>\n",
       "      <td>2.477662</td>\n",
       "      <td>-1.584504</td>\n",
       "      <td>0.761308</td>\n",
       "      <td>2.033428</td>\n",
       "      <td>1.368133</td>\n",
       "      <td>1.950793</td>\n",
       "      <td>...</td>\n",
       "      <td>3.134311</td>\n",
       "      <td>1.916011</td>\n",
       "      <td>2.712826</td>\n",
       "      <td>2.521865</td>\n",
       "      <td>2.295721</td>\n",
       "      <td>-0.997785</td>\n",
       "      <td>-0.635909</td>\n",
       "      <td>1.012655</td>\n",
       "      <td>-0.690904</td>\n",
       "      <td>-0.699789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.156497</td>\n",
       "      <td>-1.532433</td>\n",
       "      <td>0.965787</td>\n",
       "      <td>-1.069540</td>\n",
       "      <td>2.462888</td>\n",
       "      <td>-1.628854</td>\n",
       "      <td>0.737691</td>\n",
       "      <td>2.003945</td>\n",
       "      <td>1.408331</td>\n",
       "      <td>1.928733</td>\n",
       "      <td>...</td>\n",
       "      <td>3.035291</td>\n",
       "      <td>1.705025</td>\n",
       "      <td>2.365489</td>\n",
       "      <td>1.447737</td>\n",
       "      <td>1.969034</td>\n",
       "      <td>-1.021983</td>\n",
       "      <td>-0.634811</td>\n",
       "      <td>1.176807</td>\n",
       "      <td>-0.687318</td>\n",
       "      <td>-0.697953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.690334</td>\n",
       "      <td>-1.037725</td>\n",
       "      <td>0.945696</td>\n",
       "      <td>-0.889877</td>\n",
       "      <td>2.434602</td>\n",
       "      <td>-1.582641</td>\n",
       "      <td>0.834653</td>\n",
       "      <td>2.237675</td>\n",
       "      <td>1.277074</td>\n",
       "      <td>1.806921</td>\n",
       "      <td>...</td>\n",
       "      <td>3.315093</td>\n",
       "      <td>1.736725</td>\n",
       "      <td>2.581648</td>\n",
       "      <td>2.408469</td>\n",
       "      <td>2.093035</td>\n",
       "      <td>0.472739</td>\n",
       "      <td>-0.657111</td>\n",
       "      <td>1.161683</td>\n",
       "      <td>-0.693171</td>\n",
       "      <td>-0.746337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.898264</td>\n",
       "      <td>-1.024951</td>\n",
       "      <td>1.287494</td>\n",
       "      <td>1.361580</td>\n",
       "      <td>2.472686</td>\n",
       "      <td>-1.565273</td>\n",
       "      <td>0.849194</td>\n",
       "      <td>2.133223</td>\n",
       "      <td>1.236691</td>\n",
       "      <td>1.883646</td>\n",
       "      <td>...</td>\n",
       "      <td>3.168231</td>\n",
       "      <td>1.876411</td>\n",
       "      <td>2.621536</td>\n",
       "      <td>2.443635</td>\n",
       "      <td>2.233280</td>\n",
       "      <td>-1.140024</td>\n",
       "      <td>-0.650606</td>\n",
       "      <td>1.147932</td>\n",
       "      <td>-0.696421</td>\n",
       "      <td>-0.732159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.201678</td>\n",
       "      <td>-1.533180</td>\n",
       "      <td>1.162108</td>\n",
       "      <td>-1.038863</td>\n",
       "      <td>2.434444</td>\n",
       "      <td>-1.523729</td>\n",
       "      <td>0.684990</td>\n",
       "      <td>1.954543</td>\n",
       "      <td>1.301708</td>\n",
       "      <td>2.037015</td>\n",
       "      <td>...</td>\n",
       "      <td>3.280769</td>\n",
       "      <td>1.690423</td>\n",
       "      <td>2.353597</td>\n",
       "      <td>2.578398</td>\n",
       "      <td>1.988251</td>\n",
       "      <td>0.539631</td>\n",
       "      <td>-0.636150</td>\n",
       "      <td>1.019989</td>\n",
       "      <td>-0.696052</td>\n",
       "      <td>-0.747646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0.703237</td>\n",
       "      <td>-1.556758</td>\n",
       "      <td>0.973881</td>\n",
       "      <td>1.270716</td>\n",
       "      <td>2.754042</td>\n",
       "      <td>-1.553458</td>\n",
       "      <td>0.765523</td>\n",
       "      <td>2.026813</td>\n",
       "      <td>1.399363</td>\n",
       "      <td>1.847537</td>\n",
       "      <td>...</td>\n",
       "      <td>3.152909</td>\n",
       "      <td>1.762470</td>\n",
       "      <td>2.474503</td>\n",
       "      <td>2.540206</td>\n",
       "      <td>2.242131</td>\n",
       "      <td>-1.184431</td>\n",
       "      <td>-0.702468</td>\n",
       "      <td>1.172194</td>\n",
       "      <td>-0.634543</td>\n",
       "      <td>-0.695807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>2.929509</td>\n",
       "      <td>-1.576989</td>\n",
       "      <td>1.253105</td>\n",
       "      <td>-0.817522</td>\n",
       "      <td>2.487001</td>\n",
       "      <td>-1.604839</td>\n",
       "      <td>0.643520</td>\n",
       "      <td>1.811805</td>\n",
       "      <td>1.433842</td>\n",
       "      <td>1.775721</td>\n",
       "      <td>...</td>\n",
       "      <td>3.025458</td>\n",
       "      <td>1.932795</td>\n",
       "      <td>2.212476</td>\n",
       "      <td>2.417553</td>\n",
       "      <td>3.533828</td>\n",
       "      <td>0.476946</td>\n",
       "      <td>-0.670283</td>\n",
       "      <td>0.991137</td>\n",
       "      <td>-0.690577</td>\n",
       "      <td>-0.709108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>1.015800</td>\n",
       "      <td>-1.489208</td>\n",
       "      <td>0.850619</td>\n",
       "      <td>1.109658</td>\n",
       "      <td>2.767300</td>\n",
       "      <td>-1.586492</td>\n",
       "      <td>0.668770</td>\n",
       "      <td>1.975403</td>\n",
       "      <td>1.460477</td>\n",
       "      <td>1.907229</td>\n",
       "      <td>...</td>\n",
       "      <td>3.162274</td>\n",
       "      <td>1.845354</td>\n",
       "      <td>2.454672</td>\n",
       "      <td>2.432776</td>\n",
       "      <td>2.221720</td>\n",
       "      <td>-0.983499</td>\n",
       "      <td>-0.661918</td>\n",
       "      <td>1.132341</td>\n",
       "      <td>-0.690713</td>\n",
       "      <td>-0.719255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0.986102</td>\n",
       "      <td>-1.083132</td>\n",
       "      <td>1.365422</td>\n",
       "      <td>1.122860</td>\n",
       "      <td>2.711808</td>\n",
       "      <td>-1.530904</td>\n",
       "      <td>0.749390</td>\n",
       "      <td>2.004778</td>\n",
       "      <td>1.360614</td>\n",
       "      <td>1.771068</td>\n",
       "      <td>...</td>\n",
       "      <td>3.367441</td>\n",
       "      <td>1.854617</td>\n",
       "      <td>2.321572</td>\n",
       "      <td>2.475233</td>\n",
       "      <td>2.263934</td>\n",
       "      <td>-1.267264</td>\n",
       "      <td>-0.377349</td>\n",
       "      <td>0.994615</td>\n",
       "      <td>-0.687809</td>\n",
       "      <td>-0.732246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>3.202355</td>\n",
       "      <td>-1.499634</td>\n",
       "      <td>1.186508</td>\n",
       "      <td>-1.132766</td>\n",
       "      <td>2.495054</td>\n",
       "      <td>-1.663431</td>\n",
       "      <td>0.623448</td>\n",
       "      <td>2.178625</td>\n",
       "      <td>1.375766</td>\n",
       "      <td>1.849863</td>\n",
       "      <td>...</td>\n",
       "      <td>3.431197</td>\n",
       "      <td>1.695094</td>\n",
       "      <td>2.201375</td>\n",
       "      <td>2.462594</td>\n",
       "      <td>3.440150</td>\n",
       "      <td>0.372193</td>\n",
       "      <td>-0.657942</td>\n",
       "      <td>1.052318</td>\n",
       "      <td>-0.694300</td>\n",
       "      <td>-0.759628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0     0.641059 -1.556056 -1.173289  1.239458  2.477662 -1.584504  0.761308   \n",
       "1     1.156497 -1.532433  0.965787 -1.069540  2.462888 -1.628854  0.737691   \n",
       "2     0.690334 -1.037725  0.945696 -0.889877  2.434602 -1.582641  0.834653   \n",
       "3     0.898264 -1.024951  1.287494  1.361580  2.472686 -1.565273  0.849194   \n",
       "4     3.201678 -1.533180  1.162108 -1.038863  2.434444 -1.523729  0.684990   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3495  0.703237 -1.556758  0.973881  1.270716  2.754042 -1.553458  0.765523   \n",
       "3496  2.929509 -1.576989  1.253105 -0.817522  2.487001 -1.604839  0.643520   \n",
       "3497  1.015800 -1.489208  0.850619  1.109658  2.767300 -1.586492  0.668770   \n",
       "3498  0.986102 -1.083132  1.365422  1.122860  2.711808 -1.530904  0.749390   \n",
       "3499  3.202355 -1.499634  1.186508 -1.132766  2.495054 -1.663431  0.623448   \n",
       "\n",
       "            X7        X8        X9  ...      X990      X991      X992  \\\n",
       "0     2.033428  1.368133  1.950793  ...  3.134311  1.916011  2.712826   \n",
       "1     2.003945  1.408331  1.928733  ...  3.035291  1.705025  2.365489   \n",
       "2     2.237675  1.277074  1.806921  ...  3.315093  1.736725  2.581648   \n",
       "3     2.133223  1.236691  1.883646  ...  3.168231  1.876411  2.621536   \n",
       "4     1.954543  1.301708  2.037015  ...  3.280769  1.690423  2.353597   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3495  2.026813  1.399363  1.847537  ...  3.152909  1.762470  2.474503   \n",
       "3496  1.811805  1.433842  1.775721  ...  3.025458  1.932795  2.212476   \n",
       "3497  1.975403  1.460477  1.907229  ...  3.162274  1.845354  2.454672   \n",
       "3498  2.004778  1.360614  1.771068  ...  3.367441  1.854617  2.321572   \n",
       "3499  2.178625  1.375766  1.849863  ...  3.431197  1.695094  2.201375   \n",
       "\n",
       "          X993      X994      X995      X996      X997      X998      X999  \n",
       "0     2.521865  2.295721 -0.997785 -0.635909  1.012655 -0.690904 -0.699789  \n",
       "1     1.447737  1.969034 -1.021983 -0.634811  1.176807 -0.687318 -0.697953  \n",
       "2     2.408469  2.093035  0.472739 -0.657111  1.161683 -0.693171 -0.746337  \n",
       "3     2.443635  2.233280 -1.140024 -0.650606  1.147932 -0.696421 -0.732159  \n",
       "4     2.578398  1.988251  0.539631 -0.636150  1.019989 -0.696052 -0.747646  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3495  2.540206  2.242131 -1.184431 -0.702468  1.172194 -0.634543 -0.695807  \n",
       "3496  2.417553  3.533828  0.476946 -0.670283  0.991137 -0.690577 -0.709108  \n",
       "3497  2.432776  2.221720 -0.983499 -0.661918  1.132341 -0.690713 -0.719255  \n",
       "3498  2.475233  2.263934 -1.267264 -0.377349  0.994615 -0.687809 -0.732246  \n",
       "3499  2.462594  3.440150  0.372193 -0.657942  1.052318 -0.694300 -0.759628  \n",
       "\n",
       "[3500 rows x 1000 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=df_test.drop(['Index'],axis=1)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cff80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_origin=df['target']\n",
    "x_origin=df.drop(['target'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d1be0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler=StandardScaler()\n",
    "std_df_train = scaler.fit_transform(x_origin)\n",
    "std_df_test2 = scaler.transform(df_test)\n",
    "\n",
    "std_df_train = pd.DataFrame(std_df_train, index=x_origin.index, columns=x_origin.columns)\n",
    "std_df_test2 = pd.DataFrame(std_df_test2, index=df_test.index, columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "865700c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X990</th>\n",
       "      <th>X991</th>\n",
       "      <th>X992</th>\n",
       "      <th>X993</th>\n",
       "      <th>X994</th>\n",
       "      <th>X995</th>\n",
       "      <th>X996</th>\n",
       "      <th>X997</th>\n",
       "      <th>X998</th>\n",
       "      <th>X999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.184849</td>\n",
       "      <td>-0.527313</td>\n",
       "      <td>-0.044496</td>\n",
       "      <td>-1.506012</td>\n",
       "      <td>-0.756786</td>\n",
       "      <td>0.478568</td>\n",
       "      <td>-0.219431</td>\n",
       "      <td>0.086250</td>\n",
       "      <td>-0.222655</td>\n",
       "      <td>0.540745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252117</td>\n",
       "      <td>0.689123</td>\n",
       "      <td>0.609217</td>\n",
       "      <td>-5.872792</td>\n",
       "      <td>-0.525689</td>\n",
       "      <td>-0.555851</td>\n",
       "      <td>-0.353523</td>\n",
       "      <td>0.626425</td>\n",
       "      <td>-0.533983</td>\n",
       "      <td>0.119024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.079390</td>\n",
       "      <td>1.569962</td>\n",
       "      <td>0.473051</td>\n",
       "      <td>0.658793</td>\n",
       "      <td>1.219387</td>\n",
       "      <td>-0.856206</td>\n",
       "      <td>-0.475863</td>\n",
       "      <td>4.892959</td>\n",
       "      <td>-0.237580</td>\n",
       "      <td>-3.570444</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616787</td>\n",
       "      <td>-0.019675</td>\n",
       "      <td>-2.917155</td>\n",
       "      <td>-0.507893</td>\n",
       "      <td>5.011563</td>\n",
       "      <td>-0.813525</td>\n",
       "      <td>2.414811</td>\n",
       "      <td>0.627165</td>\n",
       "      <td>3.650187</td>\n",
       "      <td>0.525503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.461787</td>\n",
       "      <td>-0.846816</td>\n",
       "      <td>0.531071</td>\n",
       "      <td>0.700547</td>\n",
       "      <td>1.386990</td>\n",
       "      <td>-0.249518</td>\n",
       "      <td>-0.422582</td>\n",
       "      <td>-0.548721</td>\n",
       "      <td>0.125006</td>\n",
       "      <td>0.039597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.544239</td>\n",
       "      <td>-0.257759</td>\n",
       "      <td>-0.090001</td>\n",
       "      <td>0.641045</td>\n",
       "      <td>0.015875</td>\n",
       "      <td>-0.732104</td>\n",
       "      <td>-0.165737</td>\n",
       "      <td>-0.230430</td>\n",
       "      <td>-0.582201</td>\n",
       "      <td>0.124393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.303365</td>\n",
       "      <td>1.291546</td>\n",
       "      <td>0.490830</td>\n",
       "      <td>0.735948</td>\n",
       "      <td>1.291531</td>\n",
       "      <td>-0.033880</td>\n",
       "      <td>-0.205640</td>\n",
       "      <td>0.214519</td>\n",
       "      <td>0.154935</td>\n",
       "      <td>0.561457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.535393</td>\n",
       "      <td>-0.040836</td>\n",
       "      <td>0.475698</td>\n",
       "      <td>0.726336</td>\n",
       "      <td>-0.361380</td>\n",
       "      <td>-0.470935</td>\n",
       "      <td>2.201734</td>\n",
       "      <td>0.458224</td>\n",
       "      <td>-0.374446</td>\n",
       "      <td>0.631834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.100144</td>\n",
       "      <td>-0.445165</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.512617</td>\n",
       "      <td>1.321731</td>\n",
       "      <td>-0.025677</td>\n",
       "      <td>-0.580526</td>\n",
       "      <td>0.102273</td>\n",
       "      <td>0.725843</td>\n",
       "      <td>-4.082630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199777</td>\n",
       "      <td>-0.223504</td>\n",
       "      <td>0.336517</td>\n",
       "      <td>-0.547793</td>\n",
       "      <td>-0.695420</td>\n",
       "      <td>-0.723429</td>\n",
       "      <td>-0.736513</td>\n",
       "      <td>0.391431</td>\n",
       "      <td>-0.542146</td>\n",
       "      <td>-0.227935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>-0.445860</td>\n",
       "      <td>1.330451</td>\n",
       "      <td>0.063397</td>\n",
       "      <td>-1.619510</td>\n",
       "      <td>-1.051601</td>\n",
       "      <td>0.421641</td>\n",
       "      <td>-0.352294</td>\n",
       "      <td>-0.260194</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.111711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261572</td>\n",
       "      <td>0.070936</td>\n",
       "      <td>-3.287915</td>\n",
       "      <td>0.583499</td>\n",
       "      <td>0.215455</td>\n",
       "      <td>-0.862799</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>0.574762</td>\n",
       "      <td>0.101751</td>\n",
       "      <td>-0.317548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>-0.392982</td>\n",
       "      <td>1.536825</td>\n",
       "      <td>0.381040</td>\n",
       "      <td>0.756495</td>\n",
       "      <td>1.175353</td>\n",
       "      <td>-0.733478</td>\n",
       "      <td>-0.310190</td>\n",
       "      <td>-0.560345</td>\n",
       "      <td>0.567529</td>\n",
       "      <td>0.701687</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656946</td>\n",
       "      <td>0.712971</td>\n",
       "      <td>0.247898</td>\n",
       "      <td>-0.522115</td>\n",
       "      <td>-0.235261</td>\n",
       "      <td>-0.441677</td>\n",
       "      <td>-0.265025</td>\n",
       "      <td>0.255770</td>\n",
       "      <td>-0.403287</td>\n",
       "      <td>-5.368146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-0.728545</td>\n",
       "      <td>1.436400</td>\n",
       "      <td>0.361047</td>\n",
       "      <td>0.723189</td>\n",
       "      <td>-0.662515</td>\n",
       "      <td>-0.345622</td>\n",
       "      <td>-0.396569</td>\n",
       "      <td>-0.610727</td>\n",
       "      <td>-0.141703</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.461103</td>\n",
       "      <td>0.092861</td>\n",
       "      <td>-3.320705</td>\n",
       "      <td>-0.572990</td>\n",
       "      <td>-0.034877</td>\n",
       "      <td>-0.429462</td>\n",
       "      <td>-0.355402</td>\n",
       "      <td>0.024368</td>\n",
       "      <td>-0.200186</td>\n",
       "      <td>-0.128639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>-0.704443</td>\n",
       "      <td>-0.529141</td>\n",
       "      <td>0.516269</td>\n",
       "      <td>-1.879902</td>\n",
       "      <td>-0.646669</td>\n",
       "      <td>0.220144</td>\n",
       "      <td>-0.554983</td>\n",
       "      <td>-0.782033</td>\n",
       "      <td>-0.164858</td>\n",
       "      <td>-0.144185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.704101</td>\n",
       "      <td>-0.287386</td>\n",
       "      <td>-3.257351</td>\n",
       "      <td>-0.437154</td>\n",
       "      <td>-0.326783</td>\n",
       "      <td>1.760186</td>\n",
       "      <td>1.965914</td>\n",
       "      <td>-0.150447</td>\n",
       "      <td>-0.724713</td>\n",
       "      <td>-0.278519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>-0.292111</td>\n",
       "      <td>-0.761624</td>\n",
       "      <td>0.314546</td>\n",
       "      <td>0.425263</td>\n",
       "      <td>-0.882141</td>\n",
       "      <td>0.546688</td>\n",
       "      <td>-0.110460</td>\n",
       "      <td>0.094867</td>\n",
       "      <td>0.333180</td>\n",
       "      <td>0.076425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209147</td>\n",
       "      <td>-0.094261</td>\n",
       "      <td>0.372375</td>\n",
       "      <td>-0.342369</td>\n",
       "      <td>0.297132</td>\n",
       "      <td>1.627813</td>\n",
       "      <td>-0.359114</td>\n",
       "      <td>-0.097267</td>\n",
       "      <td>-0.111234</td>\n",
       "      <td>0.069402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0    -0.184849 -0.527313 -0.044496 -1.506012 -0.756786  0.478568 -0.219431   \n",
       "1    -0.079390  1.569962  0.473051  0.658793  1.219387 -0.856206 -0.475863   \n",
       "2    -0.461787 -0.846816  0.531071  0.700547  1.386990 -0.249518 -0.422582   \n",
       "3     3.303365  1.291546  0.490830  0.735948  1.291531 -0.033880 -0.205640   \n",
       "4     0.100144 -0.445165  0.343434  0.512617  1.321731 -0.025677 -0.580526   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4995 -0.445860  1.330451  0.063397 -1.619510 -1.051601  0.421641 -0.352294   \n",
       "4996 -0.392982  1.536825  0.381040  0.756495  1.175353 -0.733478 -0.310190   \n",
       "4997 -0.728545  1.436400  0.361047  0.723189 -0.662515 -0.345622 -0.396569   \n",
       "4998 -0.704443 -0.529141  0.516269 -1.879902 -0.646669  0.220144 -0.554983   \n",
       "4999 -0.292111 -0.761624  0.314546  0.425263 -0.882141  0.546688 -0.110460   \n",
       "\n",
       "            X7        X8        X9  ...      X990      X991      X992  \\\n",
       "0     0.086250 -0.222655  0.540745  ... -0.252117  0.689123  0.609217   \n",
       "1     4.892959 -0.237580 -3.570444  ... -0.616787 -0.019675 -2.917155   \n",
       "2    -0.548721  0.125006  0.039597  ... -0.544239 -0.257759 -0.090001   \n",
       "3     0.214519  0.154935  0.561457  ... -0.535393 -0.040836  0.475698   \n",
       "4     0.102273  0.725843 -4.082630  ... -0.199777 -0.223504  0.336517   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4995 -0.260194  0.707668  0.111711  ...  0.261572  0.070936 -3.287915   \n",
       "4996 -0.560345  0.567529  0.701687  ... -0.656946  0.712971  0.247898   \n",
       "4997 -0.610727 -0.141703  0.189944  ... -0.461103  0.092861 -3.320705   \n",
       "4998 -0.782033 -0.164858 -0.144185  ... -0.704101 -0.287386 -3.257351   \n",
       "4999  0.094867  0.333180  0.076425  ...  0.209147 -0.094261  0.372375   \n",
       "\n",
       "          X993      X994      X995      X996      X997      X998      X999  \n",
       "0    -5.872792 -0.525689 -0.555851 -0.353523  0.626425 -0.533983  0.119024  \n",
       "1    -0.507893  5.011563 -0.813525  2.414811  0.627165  3.650187  0.525503  \n",
       "2     0.641045  0.015875 -0.732104 -0.165737 -0.230430 -0.582201  0.124393  \n",
       "3     0.726336 -0.361380 -0.470935  2.201734  0.458224 -0.374446  0.631834  \n",
       "4    -0.547793 -0.695420 -0.723429 -0.736513  0.391431 -0.542146 -0.227935  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4995  0.583499  0.215455 -0.862799 -0.460886  0.574762  0.101751 -0.317548  \n",
       "4996 -0.522115 -0.235261 -0.441677 -0.265025  0.255770 -0.403287 -5.368146  \n",
       "4997 -0.572990 -0.034877 -0.429462 -0.355402  0.024368 -0.200186 -0.128639  \n",
       "4998 -0.437154 -0.326783  1.760186  1.965914 -0.150447 -0.724713 -0.278519  \n",
       "4999 -0.342369  0.297132  1.627813 -0.359114 -0.097267 -0.111234  0.069402  \n",
       "\n",
       "[5000 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b8c784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(std_df_train,y_origin,test_size=0.2,stratify=y_origin,random_state=112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dc8fb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X990</th>\n",
       "      <th>X991</th>\n",
       "      <th>X992</th>\n",
       "      <th>X993</th>\n",
       "      <th>X994</th>\n",
       "      <th>X995</th>\n",
       "      <th>X996</th>\n",
       "      <th>X997</th>\n",
       "      <th>X998</th>\n",
       "      <th>X999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.626339</td>\n",
       "      <td>1.436034</td>\n",
       "      <td>-0.108864</td>\n",
       "      <td>0.469629</td>\n",
       "      <td>1.306236</td>\n",
       "      <td>0.121529</td>\n",
       "      <td>-0.133967</td>\n",
       "      <td>-0.633675</td>\n",
       "      <td>0.632659</td>\n",
       "      <td>0.249392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634312</td>\n",
       "      <td>0.369388</td>\n",
       "      <td>0.037421</td>\n",
       "      <td>0.320119</td>\n",
       "      <td>0.395351</td>\n",
       "      <td>-0.575731</td>\n",
       "      <td>-0.541946</td>\n",
       "      <td>0.717594</td>\n",
       "      <td>-0.364531</td>\n",
       "      <td>0.426638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152210</td>\n",
       "      <td>-0.419177</td>\n",
       "      <td>0.382479</td>\n",
       "      <td>0.868608</td>\n",
       "      <td>-0.934346</td>\n",
       "      <td>-0.657295</td>\n",
       "      <td>0.018058</td>\n",
       "      <td>-0.790259</td>\n",
       "      <td>-3.441347</td>\n",
       "      <td>-0.212017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138250</td>\n",
       "      <td>0.496670</td>\n",
       "      <td>0.214721</td>\n",
       "      <td>-0.582664</td>\n",
       "      <td>-0.568832</td>\n",
       "      <td>1.539021</td>\n",
       "      <td>-0.456686</td>\n",
       "      <td>-0.024628</td>\n",
       "      <td>-0.618522</td>\n",
       "      <td>0.371864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.713403</td>\n",
       "      <td>1.307859</td>\n",
       "      <td>-0.118133</td>\n",
       "      <td>0.835869</td>\n",
       "      <td>1.177788</td>\n",
       "      <td>-0.620360</td>\n",
       "      <td>-0.532650</td>\n",
       "      <td>0.273697</td>\n",
       "      <td>0.567431</td>\n",
       "      <td>-0.122512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017555</td>\n",
       "      <td>0.047373</td>\n",
       "      <td>0.133732</td>\n",
       "      <td>-0.017794</td>\n",
       "      <td>0.164679</td>\n",
       "      <td>-0.717401</td>\n",
       "      <td>-0.739594</td>\n",
       "      <td>0.101792</td>\n",
       "      <td>-0.347780</td>\n",
       "      <td>0.349256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.102191</td>\n",
       "      <td>-0.409136</td>\n",
       "      <td>-0.034177</td>\n",
       "      <td>0.590975</td>\n",
       "      <td>1.387047</td>\n",
       "      <td>-0.138057</td>\n",
       "      <td>-0.017228</td>\n",
       "      <td>-0.438758</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.127995</td>\n",
       "      <td>...</td>\n",
       "      <td>4.275757</td>\n",
       "      <td>0.252544</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>-0.612028</td>\n",
       "      <td>-0.355405</td>\n",
       "      <td>-0.450141</td>\n",
       "      <td>-0.753407</td>\n",
       "      <td>-4.179275</td>\n",
       "      <td>-0.277547</td>\n",
       "      <td>0.197572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.483186</td>\n",
       "      <td>-0.900152</td>\n",
       "      <td>0.159229</td>\n",
       "      <td>0.856172</td>\n",
       "      <td>0.998276</td>\n",
       "      <td>-0.025667</td>\n",
       "      <td>-0.681478</td>\n",
       "      <td>-0.562245</td>\n",
       "      <td>0.548763</td>\n",
       "      <td>0.335777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319843</td>\n",
       "      <td>0.314420</td>\n",
       "      <td>0.396648</td>\n",
       "      <td>0.636865</td>\n",
       "      <td>0.042011</td>\n",
       "      <td>-0.394890</td>\n",
       "      <td>-0.628083</td>\n",
       "      <td>-0.245963</td>\n",
       "      <td>-0.433789</td>\n",
       "      <td>-0.037472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295</th>\n",
       "      <td>-0.192885</td>\n",
       "      <td>0.975833</td>\n",
       "      <td>0.426418</td>\n",
       "      <td>0.558857</td>\n",
       "      <td>1.056107</td>\n",
       "      <td>0.413424</td>\n",
       "      <td>-0.643689</td>\n",
       "      <td>-0.364047</td>\n",
       "      <td>0.223127</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.274189</td>\n",
       "      <td>-0.151750</td>\n",
       "      <td>0.618657</td>\n",
       "      <td>0.361821</td>\n",
       "      <td>-0.344433</td>\n",
       "      <td>-0.717324</td>\n",
       "      <td>-0.299993</td>\n",
       "      <td>-0.029564</td>\n",
       "      <td>-0.480783</td>\n",
       "      <td>-0.002511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296</th>\n",
       "      <td>-0.516751</td>\n",
       "      <td>-0.752427</td>\n",
       "      <td>0.322733</td>\n",
       "      <td>0.644945</td>\n",
       "      <td>0.366923</td>\n",
       "      <td>-0.071632</td>\n",
       "      <td>-0.536363</td>\n",
       "      <td>-0.224544</td>\n",
       "      <td>0.221699</td>\n",
       "      <td>0.317456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.253267</td>\n",
       "      <td>0.342652</td>\n",
       "      <td>0.183591</td>\n",
       "      <td>0.397569</td>\n",
       "      <td>-0.318817</td>\n",
       "      <td>-0.489233</td>\n",
       "      <td>-0.450391</td>\n",
       "      <td>0.307651</td>\n",
       "      <td>-0.431824</td>\n",
       "      <td>-0.140249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>-0.526927</td>\n",
       "      <td>-0.851531</td>\n",
       "      <td>0.052461</td>\n",
       "      <td>-1.644034</td>\n",
       "      <td>-0.635921</td>\n",
       "      <td>-0.513466</td>\n",
       "      <td>2.343101</td>\n",
       "      <td>-0.117010</td>\n",
       "      <td>-0.093142</td>\n",
       "      <td>0.055768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334512</td>\n",
       "      <td>-0.248261</td>\n",
       "      <td>-2.843128</td>\n",
       "      <td>0.462418</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>1.674518</td>\n",
       "      <td>2.480095</td>\n",
       "      <td>-0.169997</td>\n",
       "      <td>-0.144866</td>\n",
       "      <td>-4.413755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>-0.485317</td>\n",
       "      <td>0.777121</td>\n",
       "      <td>0.296526</td>\n",
       "      <td>-1.602088</td>\n",
       "      <td>-0.654455</td>\n",
       "      <td>-0.502910</td>\n",
       "      <td>-0.312477</td>\n",
       "      <td>-0.134729</td>\n",
       "      <td>0.100428</td>\n",
       "      <td>-0.033673</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251282</td>\n",
       "      <td>0.038632</td>\n",
       "      <td>0.237814</td>\n",
       "      <td>0.364760</td>\n",
       "      <td>-0.132697</td>\n",
       "      <td>1.120917</td>\n",
       "      <td>0.244602</td>\n",
       "      <td>0.289978</td>\n",
       "      <td>-0.207707</td>\n",
       "      <td>0.091943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>-0.208625</td>\n",
       "      <td>-0.646535</td>\n",
       "      <td>0.418882</td>\n",
       "      <td>0.669903</td>\n",
       "      <td>-0.098142</td>\n",
       "      <td>-0.191644</td>\n",
       "      <td>-0.572329</td>\n",
       "      <td>-0.087116</td>\n",
       "      <td>0.121557</td>\n",
       "      <td>-0.028925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.490942</td>\n",
       "      <td>0.249821</td>\n",
       "      <td>0.615912</td>\n",
       "      <td>0.400436</td>\n",
       "      <td>0.055618</td>\n",
       "      <td>-0.728759</td>\n",
       "      <td>-0.585248</td>\n",
       "      <td>0.424345</td>\n",
       "      <td>-0.191932</td>\n",
       "      <td>0.670711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7300 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X0        X1        X2        X3        X4        X5        X6  \\\n",
       "0    -0.626339  1.436034 -0.108864  0.469629  1.306236  0.121529 -0.133967   \n",
       "1     0.152210 -0.419177  0.382479  0.868608 -0.934346 -0.657295  0.018058   \n",
       "2    -0.713403  1.307859 -0.118133  0.835869  1.177788 -0.620360 -0.532650   \n",
       "3     0.102191 -0.409136 -0.034177  0.590975  1.387047 -0.138057 -0.017228   \n",
       "4    -0.483186 -0.900152  0.159229  0.856172  0.998276 -0.025667 -0.681478   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7295 -0.192885  0.975833  0.426418  0.558857  1.056107  0.413424 -0.643689   \n",
       "7296 -0.516751 -0.752427  0.322733  0.644945  0.366923 -0.071632 -0.536363   \n",
       "7297 -0.526927 -0.851531  0.052461 -1.644034 -0.635921 -0.513466  2.343101   \n",
       "7298 -0.485317  0.777121  0.296526 -1.602088 -0.654455 -0.502910 -0.312477   \n",
       "7299 -0.208625 -0.646535  0.418882  0.669903 -0.098142 -0.191644 -0.572329   \n",
       "\n",
       "            X7        X8        X9  ...      X990      X991      X992  \\\n",
       "0    -0.633675  0.632659  0.249392  ... -0.634312  0.369388  0.037421   \n",
       "1    -0.790259 -3.441347 -0.212017  ... -0.138250  0.496670  0.214721   \n",
       "2     0.273697  0.567431 -0.122512  ...  0.017555  0.047373  0.133732   \n",
       "3    -0.438758  0.408248  0.127995  ...  4.275757  0.252544  0.439300   \n",
       "4    -0.562245  0.548763  0.335777  ...  0.319843  0.314420  0.396648   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7295 -0.364047  0.223127  0.003392  ... -0.274189 -0.151750  0.618657   \n",
       "7296 -0.224544  0.221699  0.317456  ... -0.253267  0.342652  0.183591   \n",
       "7297 -0.117010 -0.093142  0.055768  ...  0.334512 -0.248261 -2.843128   \n",
       "7298 -0.134729  0.100428 -0.033673  ... -0.251282  0.038632  0.237814   \n",
       "7299 -0.087116  0.121557 -0.028925  ... -0.490942  0.249821  0.615912   \n",
       "\n",
       "          X993      X994      X995      X996      X997      X998      X999  \n",
       "0     0.320119  0.395351 -0.575731 -0.541946  0.717594 -0.364531  0.426638  \n",
       "1    -0.582664 -0.568832  1.539021 -0.456686 -0.024628 -0.618522  0.371864  \n",
       "2    -0.017794  0.164679 -0.717401 -0.739594  0.101792 -0.347780  0.349256  \n",
       "3    -0.612028 -0.355405 -0.450141 -0.753407 -4.179275 -0.277547  0.197572  \n",
       "4     0.636865  0.042011 -0.394890 -0.628083 -0.245963 -0.433789 -0.037472  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7295  0.361821 -0.344433 -0.717324 -0.299993 -0.029564 -0.480783 -0.002511  \n",
       "7296  0.397569 -0.318817 -0.489233 -0.450391  0.307651 -0.431824 -0.140249  \n",
       "7297  0.462418  0.066787  1.674518  2.480095 -0.169997 -0.144866 -4.413755  \n",
       "7298  0.364760 -0.132697  1.120917  0.244602  0.289978 -0.207707  0.091943  \n",
       "7299  0.400436  0.055618 -0.728759 -0.585248  0.424345 -0.191932  0.670711  \n",
       "\n",
       "[7300 rows x 1000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks#오버샘플링\n",
    "\n",
    "smoteto = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'),random_state=112)\n",
    "X_train_over, Y_train_over = smoteto.fit_sample(X_train, y_train)\n",
    "X_train_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3798c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=240,random_state=112)\n",
    "pca_train = pca.fit_transform(X_train_over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d495ac50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.267083</td>\n",
       "      <td>1.884290</td>\n",
       "      <td>0.524454</td>\n",
       "      <td>-3.033502</td>\n",
       "      <td>2.259334</td>\n",
       "      <td>-1.327294</td>\n",
       "      <td>-0.296643</td>\n",
       "      <td>-0.436208</td>\n",
       "      <td>-0.257987</td>\n",
       "      <td>0.414217</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.197685</td>\n",
       "      <td>-0.111031</td>\n",
       "      <td>-0.633513</td>\n",
       "      <td>-0.523722</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>-0.787292</td>\n",
       "      <td>-2.299620</td>\n",
       "      <td>0.443204</td>\n",
       "      <td>-0.838725</td>\n",
       "      <td>-0.606501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.596031</td>\n",
       "      <td>5.644906</td>\n",
       "      <td>3.842402</td>\n",
       "      <td>0.349073</td>\n",
       "      <td>-4.929387</td>\n",
       "      <td>-2.546235</td>\n",
       "      <td>-1.137812</td>\n",
       "      <td>2.710533</td>\n",
       "      <td>-1.057311</td>\n",
       "      <td>-0.079239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115436</td>\n",
       "      <td>1.068694</td>\n",
       "      <td>-0.701503</td>\n",
       "      <td>0.645031</td>\n",
       "      <td>2.188521</td>\n",
       "      <td>2.225453</td>\n",
       "      <td>0.469560</td>\n",
       "      <td>2.205811</td>\n",
       "      <td>-0.198809</td>\n",
       "      <td>1.658348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.970043</td>\n",
       "      <td>3.775527</td>\n",
       "      <td>3.495793</td>\n",
       "      <td>-2.968689</td>\n",
       "      <td>0.837182</td>\n",
       "      <td>-2.460759</td>\n",
       "      <td>-3.705652</td>\n",
       "      <td>1.046119</td>\n",
       "      <td>0.263928</td>\n",
       "      <td>2.095799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366574</td>\n",
       "      <td>-0.108274</td>\n",
       "      <td>-0.467861</td>\n",
       "      <td>0.061978</td>\n",
       "      <td>-0.945135</td>\n",
       "      <td>-0.855857</td>\n",
       "      <td>-0.813064</td>\n",
       "      <td>-1.315119</td>\n",
       "      <td>0.011259</td>\n",
       "      <td>-0.823885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.210392</td>\n",
       "      <td>11.126614</td>\n",
       "      <td>9.849883</td>\n",
       "      <td>2.480056</td>\n",
       "      <td>-1.631613</td>\n",
       "      <td>3.043894</td>\n",
       "      <td>4.864732</td>\n",
       "      <td>-3.357467</td>\n",
       "      <td>1.919231</td>\n",
       "      <td>-6.404746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343410</td>\n",
       "      <td>-1.387552</td>\n",
       "      <td>-0.514140</td>\n",
       "      <td>1.917162</td>\n",
       "      <td>0.115717</td>\n",
       "      <td>0.167354</td>\n",
       "      <td>2.167461</td>\n",
       "      <td>1.249635</td>\n",
       "      <td>1.005148</td>\n",
       "      <td>-1.673497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.417087</td>\n",
       "      <td>5.633709</td>\n",
       "      <td>2.833012</td>\n",
       "      <td>-2.909625</td>\n",
       "      <td>0.863046</td>\n",
       "      <td>-1.696924</td>\n",
       "      <td>0.424963</td>\n",
       "      <td>-3.352273</td>\n",
       "      <td>1.624549</td>\n",
       "      <td>0.963894</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054285</td>\n",
       "      <td>-1.405884</td>\n",
       "      <td>-0.876206</td>\n",
       "      <td>1.100636</td>\n",
       "      <td>1.727116</td>\n",
       "      <td>-1.581462</td>\n",
       "      <td>-0.513174</td>\n",
       "      <td>-2.065635</td>\n",
       "      <td>0.945060</td>\n",
       "      <td>0.259194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pca1       pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0 -4.267083   1.884290  0.524454 -3.033502  2.259334 -1.327294 -0.296643   \n",
       "1  3.596031   5.644906  3.842402  0.349073 -4.929387 -2.546235 -1.137812   \n",
       "2 -2.970043   3.775527  3.495793 -2.968689  0.837182 -2.460759 -3.705652   \n",
       "3  2.210392  11.126614  9.849883  2.480056 -1.631613  3.043894  4.864732   \n",
       "4 -1.417087   5.633709  2.833012 -2.909625  0.863046 -1.696924  0.424963   \n",
       "\n",
       "       pca8      pca9     pca10  ...    pca231    pca232    pca233    pca234  \\\n",
       "0 -0.436208 -0.257987  0.414217  ... -1.197685 -0.111031 -0.633513 -0.523722   \n",
       "1  2.710533 -1.057311 -0.079239  ...  0.115436  1.068694 -0.701503  0.645031   \n",
       "2  1.046119  0.263928  2.095799  ... -0.366574 -0.108274 -0.467861  0.061978   \n",
       "3 -3.357467  1.919231 -6.404746  ...  0.343410 -1.387552 -0.514140  1.917162   \n",
       "4 -3.352273  1.624549  0.963894  ...  1.054285 -1.405884 -0.876206  1.100636   \n",
       "\n",
       "     pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "0  0.014635 -0.787292 -2.299620  0.443204 -0.838725 -0.606501  \n",
       "1  2.188521  2.225453  0.469560  2.205811 -0.198809  1.658348  \n",
       "2 -0.945135 -0.855857 -0.813064 -1.315119  0.011259 -0.823885  \n",
       "3  0.115717  0.167354  2.167461  1.249635  1.005148 -1.673497  \n",
       "4  1.727116 -1.581462 -0.513174 -2.065635  0.945060  0.259194  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_train = pd.DataFrame(pca_train, index=X_train_over.index,columns=[f\"pca{num+1}\" for num in range(0,240)])\n",
    "pca_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f130f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>설명가능한 분산 비율(고윳값)</th>\n",
       "      <th>기여율</th>\n",
       "      <th>누적기여율</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pca1</th>\n",
       "      <td>35.977233</td>\n",
       "      <td>0.042085</td>\n",
       "      <td>0.042085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca2</th>\n",
       "      <td>33.834213</td>\n",
       "      <td>0.039578</td>\n",
       "      <td>0.081663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca3</th>\n",
       "      <td>23.704788</td>\n",
       "      <td>0.027729</td>\n",
       "      <td>0.109392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca4</th>\n",
       "      <td>21.118933</td>\n",
       "      <td>0.024704</td>\n",
       "      <td>0.134097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca5</th>\n",
       "      <td>15.707395</td>\n",
       "      <td>0.018374</td>\n",
       "      <td>0.152471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca236</th>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.729087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca237</th>\n",
       "      <td>0.776849</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.729996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca238</th>\n",
       "      <td>0.770669</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.730897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca239</th>\n",
       "      <td>0.762052</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.731789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pca240</th>\n",
       "      <td>0.757971</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.732675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        설명가능한 분산 비율(고윳값)       기여율     누적기여율\n",
       "pca1           35.977233  0.042085  0.042085\n",
       "pca2           33.834213  0.039578  0.081663\n",
       "pca3           23.704788  0.027729  0.109392\n",
       "pca4           21.118933  0.024704  0.134097\n",
       "pca5           15.707395  0.018374  0.152471\n",
       "...                  ...       ...       ...\n",
       "pca236          0.777174  0.000909  0.729087\n",
       "pca237          0.776849  0.000909  0.729996\n",
       "pca238          0.770669  0.000902  0.730897\n",
       "pca239          0.762052  0.000891  0.731789\n",
       "pca240          0.757971  0.000887  0.732675\n",
       "\n",
       "[240 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2 = pd.DataFrame({'설명가능한 분산 비율(고윳값)':pca.explained_variance_,\n",
    "             '기여율':pca.explained_variance_ratio_},\n",
    "            index=np.array([f\"pca{num+1}\" for num in range(0,240)]))\n",
    "result2['누적기여율'] = result2['기여율'].cumsum()\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b01915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = pca.transform(X_test)\n",
    "pca_test2= pca.transform(std_df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a99df36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>-3.601599</td>\n",
       "      <td>3.772858</td>\n",
       "      <td>2.521583</td>\n",
       "      <td>-3.478029</td>\n",
       "      <td>1.697557</td>\n",
       "      <td>1.070865</td>\n",
       "      <td>0.611804</td>\n",
       "      <td>-3.308214</td>\n",
       "      <td>1.939259</td>\n",
       "      <td>-3.707263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143809</td>\n",
       "      <td>1.100848</td>\n",
       "      <td>-0.456213</td>\n",
       "      <td>0.629040</td>\n",
       "      <td>0.501644</td>\n",
       "      <td>-0.298930</td>\n",
       "      <td>1.904883</td>\n",
       "      <td>-1.767327</td>\n",
       "      <td>1.226648</td>\n",
       "      <td>1.103779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4778</th>\n",
       "      <td>-0.535072</td>\n",
       "      <td>3.838789</td>\n",
       "      <td>-7.384069</td>\n",
       "      <td>9.736512</td>\n",
       "      <td>-3.148387</td>\n",
       "      <td>-1.427562</td>\n",
       "      <td>0.565839</td>\n",
       "      <td>-1.284952</td>\n",
       "      <td>1.201034</td>\n",
       "      <td>-4.480312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537868</td>\n",
       "      <td>-0.135754</td>\n",
       "      <td>-0.399703</td>\n",
       "      <td>0.700984</td>\n",
       "      <td>-1.787900</td>\n",
       "      <td>1.609352</td>\n",
       "      <td>1.541595</td>\n",
       "      <td>-0.268386</td>\n",
       "      <td>-2.647097</td>\n",
       "      <td>0.376761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>6.824900</td>\n",
       "      <td>3.146462</td>\n",
       "      <td>-1.992341</td>\n",
       "      <td>-0.772107</td>\n",
       "      <td>-2.545391</td>\n",
       "      <td>1.350090</td>\n",
       "      <td>-3.282361</td>\n",
       "      <td>-2.600658</td>\n",
       "      <td>0.243475</td>\n",
       "      <td>-2.273036</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.961030</td>\n",
       "      <td>0.588105</td>\n",
       "      <td>0.158627</td>\n",
       "      <td>1.225371</td>\n",
       "      <td>-0.566169</td>\n",
       "      <td>0.781788</td>\n",
       "      <td>1.427164</td>\n",
       "      <td>0.535400</td>\n",
       "      <td>-1.894775</td>\n",
       "      <td>-0.228519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>-1.827833</td>\n",
       "      <td>-1.767213</td>\n",
       "      <td>-5.493652</td>\n",
       "      <td>1.534201</td>\n",
       "      <td>-2.172781</td>\n",
       "      <td>-1.684911</td>\n",
       "      <td>-0.578087</td>\n",
       "      <td>2.773667</td>\n",
       "      <td>-2.578790</td>\n",
       "      <td>-3.443398</td>\n",
       "      <td>...</td>\n",
       "      <td>1.367283</td>\n",
       "      <td>-0.823645</td>\n",
       "      <td>0.956216</td>\n",
       "      <td>-0.199482</td>\n",
       "      <td>-0.002739</td>\n",
       "      <td>0.627226</td>\n",
       "      <td>-0.548607</td>\n",
       "      <td>0.167715</td>\n",
       "      <td>-0.088966</td>\n",
       "      <td>-0.944775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>4.778250</td>\n",
       "      <td>-4.577262</td>\n",
       "      <td>-4.498279</td>\n",
       "      <td>-3.025209</td>\n",
       "      <td>-2.299402</td>\n",
       "      <td>5.130555</td>\n",
       "      <td>-1.724824</td>\n",
       "      <td>-1.705525</td>\n",
       "      <td>-1.418568</td>\n",
       "      <td>-1.800972</td>\n",
       "      <td>...</td>\n",
       "      <td>1.372431</td>\n",
       "      <td>0.330504</td>\n",
       "      <td>0.497548</td>\n",
       "      <td>0.431873</td>\n",
       "      <td>0.243818</td>\n",
       "      <td>1.381109</td>\n",
       "      <td>0.872241</td>\n",
       "      <td>0.852545</td>\n",
       "      <td>0.176810</td>\n",
       "      <td>0.905082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "3314 -3.601599  3.772858  2.521583 -3.478029  1.697557  1.070865  0.611804   \n",
       "4778 -0.535072  3.838789 -7.384069  9.736512 -3.148387 -1.427562  0.565839   \n",
       "859   6.824900  3.146462 -1.992341 -0.772107 -2.545391  1.350090 -3.282361   \n",
       "2656 -1.827833 -1.767213 -5.493652  1.534201 -2.172781 -1.684911 -0.578087   \n",
       "1482  4.778250 -4.577262 -4.498279 -3.025209 -2.299402  5.130555 -1.724824   \n",
       "\n",
       "          pca8      pca9     pca10  ...    pca231    pca232    pca233  \\\n",
       "3314 -3.308214  1.939259 -3.707263  ...  1.143809  1.100848 -0.456213   \n",
       "4778 -1.284952  1.201034 -4.480312  ...  0.537868 -0.135754 -0.399703   \n",
       "859  -2.600658  0.243475 -2.273036  ... -0.961030  0.588105  0.158627   \n",
       "2656  2.773667 -2.578790 -3.443398  ...  1.367283 -0.823645  0.956216   \n",
       "1482 -1.705525 -1.418568 -1.800972  ...  1.372431  0.330504  0.497548   \n",
       "\n",
       "        pca234    pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "3314  0.629040  0.501644 -0.298930  1.904883 -1.767327  1.226648  1.103779  \n",
       "4778  0.700984 -1.787900  1.609352  1.541595 -0.268386 -2.647097  0.376761  \n",
       "859   1.225371 -0.566169  0.781788  1.427164  0.535400 -1.894775 -0.228519  \n",
       "2656 -0.199482 -0.002739  0.627226 -0.548607  0.167715 -0.088966 -0.944775  \n",
       "1482  0.431873  0.243818  1.381109  0.872241  0.852545  0.176810  0.905082  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_test = pd.DataFrame(pca_test, index=X_test.index,columns=[f\"pca{num+1}\" for num in range(0,240)])\n",
    "pca_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0202a9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.715317</td>\n",
       "      <td>0.227776</td>\n",
       "      <td>0.352166</td>\n",
       "      <td>1.610915</td>\n",
       "      <td>0.869998</td>\n",
       "      <td>1.799627</td>\n",
       "      <td>3.209160</td>\n",
       "      <td>-6.775694</td>\n",
       "      <td>1.764396</td>\n",
       "      <td>0.317379</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.518880</td>\n",
       "      <td>-1.110453</td>\n",
       "      <td>0.539658</td>\n",
       "      <td>0.081394</td>\n",
       "      <td>0.959968</td>\n",
       "      <td>-0.094326</td>\n",
       "      <td>2.012685</td>\n",
       "      <td>-1.391976</td>\n",
       "      <td>0.111931</td>\n",
       "      <td>2.108575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.779939</td>\n",
       "      <td>-3.098362</td>\n",
       "      <td>-5.074701</td>\n",
       "      <td>-2.916885</td>\n",
       "      <td>0.769355</td>\n",
       "      <td>0.935954</td>\n",
       "      <td>0.114798</td>\n",
       "      <td>-2.664788</td>\n",
       "      <td>-2.946693</td>\n",
       "      <td>0.911761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376822</td>\n",
       "      <td>0.572733</td>\n",
       "      <td>-1.283232</td>\n",
       "      <td>0.254403</td>\n",
       "      <td>-0.964710</td>\n",
       "      <td>-0.794650</td>\n",
       "      <td>-0.551161</td>\n",
       "      <td>-0.528391</td>\n",
       "      <td>0.487733</td>\n",
       "      <td>-1.082123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.452717</td>\n",
       "      <td>0.680779</td>\n",
       "      <td>-4.922382</td>\n",
       "      <td>-2.838709</td>\n",
       "      <td>-0.096270</td>\n",
       "      <td>-3.552812</td>\n",
       "      <td>-5.315669</td>\n",
       "      <td>2.041356</td>\n",
       "      <td>-4.190242</td>\n",
       "      <td>0.262984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.796941</td>\n",
       "      <td>0.175031</td>\n",
       "      <td>0.345269</td>\n",
       "      <td>-1.060643</td>\n",
       "      <td>0.766865</td>\n",
       "      <td>0.803322</td>\n",
       "      <td>0.169110</td>\n",
       "      <td>-1.492672</td>\n",
       "      <td>0.884509</td>\n",
       "      <td>-0.176871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.259551</td>\n",
       "      <td>5.010646</td>\n",
       "      <td>-7.006775</td>\n",
       "      <td>9.844512</td>\n",
       "      <td>-0.800300</td>\n",
       "      <td>-2.817524</td>\n",
       "      <td>-3.857614</td>\n",
       "      <td>0.034317</td>\n",
       "      <td>-2.912894</td>\n",
       "      <td>-1.077108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.247253</td>\n",
       "      <td>-1.936415</td>\n",
       "      <td>0.525888</td>\n",
       "      <td>-0.213031</td>\n",
       "      <td>1.303784</td>\n",
       "      <td>1.556064</td>\n",
       "      <td>0.916562</td>\n",
       "      <td>0.343616</td>\n",
       "      <td>0.608192</td>\n",
       "      <td>1.231565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.731713</td>\n",
       "      <td>-1.017324</td>\n",
       "      <td>-7.715286</td>\n",
       "      <td>-2.460875</td>\n",
       "      <td>1.922055</td>\n",
       "      <td>-1.966755</td>\n",
       "      <td>-2.176315</td>\n",
       "      <td>-1.158903</td>\n",
       "      <td>-2.985497</td>\n",
       "      <td>-0.950455</td>\n",
       "      <td>...</td>\n",
       "      <td>1.174822</td>\n",
       "      <td>-2.023138</td>\n",
       "      <td>0.856609</td>\n",
       "      <td>-0.044623</td>\n",
       "      <td>-0.008036</td>\n",
       "      <td>1.504113</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>1.317932</td>\n",
       "      <td>-0.681646</td>\n",
       "      <td>-1.405812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0   7.715317  0.227776  0.352166  1.610915  0.869998  1.799627  3.209160   \n",
       "1   4.779939 -3.098362 -5.074701 -2.916885  0.769355  0.935954  0.114798   \n",
       "2   8.452717  0.680779 -4.922382 -2.838709 -0.096270 -3.552812 -5.315669   \n",
       "3  -1.259551  5.010646 -7.006775  9.844512 -0.800300 -2.817524 -3.857614   \n",
       "4  10.731713 -1.017324 -7.715286 -2.460875  1.922055 -1.966755 -2.176315   \n",
       "\n",
       "       pca8      pca9     pca10  ...    pca231    pca232    pca233    pca234  \\\n",
       "0 -6.775694  1.764396  0.317379  ... -1.518880 -1.110453  0.539658  0.081394   \n",
       "1 -2.664788 -2.946693  0.911761  ...  0.376822  0.572733 -1.283232  0.254403   \n",
       "2  2.041356 -4.190242  0.262984  ... -0.796941  0.175031  0.345269 -1.060643   \n",
       "3  0.034317 -2.912894 -1.077108  ... -0.247253 -1.936415  0.525888 -0.213031   \n",
       "4 -1.158903 -2.985497 -0.950455  ...  1.174822 -2.023138  0.856609 -0.044623   \n",
       "\n",
       "     pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "0  0.959968 -0.094326  2.012685 -1.391976  0.111931  2.108575  \n",
       "1 -0.964710 -0.794650 -0.551161 -0.528391  0.487733 -1.082123  \n",
       "2  0.766865  0.803322  0.169110 -1.492672  0.884509 -0.176871  \n",
       "3  1.303784  1.556064  0.916562  0.343616  0.608192  1.231565  \n",
       "4 -0.008036  1.504113  0.006264  1.317932 -0.681646 -1.405812  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_test2 = pd.DataFrame(pca_test2, index=df_test.index,columns=[f\"pca{num+1}\" for num in range(0,240)])\n",
    "pca_test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1320297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(pca_train) \n",
    "X_test_scaled = scaler.transform(pca_test) \n",
    "X_test2_scaled= scaler.transform(pca_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a61fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_=pd.DataFrame(X_train_scaled,columns=pca_train.columns)\n",
    "X_test_=pd.DataFrame(X_test_scaled,columns=pca_test.columns)\n",
    "X_test2=pd.DataFrame(X_test2_scaled,columns=pca_test2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5234e8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.116150</td>\n",
       "      <td>0.515373</td>\n",
       "      <td>0.306004</td>\n",
       "      <td>0.136208</td>\n",
       "      <td>0.346313</td>\n",
       "      <td>0.233494</td>\n",
       "      <td>0.351407</td>\n",
       "      <td>0.464060</td>\n",
       "      <td>0.454801</td>\n",
       "      <td>0.500294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396519</td>\n",
       "      <td>0.480992</td>\n",
       "      <td>0.386971</td>\n",
       "      <td>0.349232</td>\n",
       "      <td>0.390425</td>\n",
       "      <td>0.407799</td>\n",
       "      <td>0.210918</td>\n",
       "      <td>0.473298</td>\n",
       "      <td>0.386899</td>\n",
       "      <td>0.397036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.299883</td>\n",
       "      <td>0.607502</td>\n",
       "      <td>0.400980</td>\n",
       "      <td>0.303032</td>\n",
       "      <td>0.174298</td>\n",
       "      <td>0.198789</td>\n",
       "      <td>0.316660</td>\n",
       "      <td>0.579717</td>\n",
       "      <td>0.432169</td>\n",
       "      <td>0.484432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506064</td>\n",
       "      <td>0.589093</td>\n",
       "      <td>0.380978</td>\n",
       "      <td>0.481628</td>\n",
       "      <td>0.564485</td>\n",
       "      <td>0.757200</td>\n",
       "      <td>0.508659</td>\n",
       "      <td>0.636983</td>\n",
       "      <td>0.457745</td>\n",
       "      <td>0.582192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.146457</td>\n",
       "      <td>0.561705</td>\n",
       "      <td>0.391059</td>\n",
       "      <td>0.139404</td>\n",
       "      <td>0.312283</td>\n",
       "      <td>0.201222</td>\n",
       "      <td>0.210588</td>\n",
       "      <td>0.518542</td>\n",
       "      <td>0.469579</td>\n",
       "      <td>0.554351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465853</td>\n",
       "      <td>0.481245</td>\n",
       "      <td>0.401573</td>\n",
       "      <td>0.415580</td>\n",
       "      <td>0.313577</td>\n",
       "      <td>0.399847</td>\n",
       "      <td>0.370752</td>\n",
       "      <td>0.310011</td>\n",
       "      <td>0.481002</td>\n",
       "      <td>0.379264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267506</td>\n",
       "      <td>0.741793</td>\n",
       "      <td>0.572944</td>\n",
       "      <td>0.408130</td>\n",
       "      <td>0.253209</td>\n",
       "      <td>0.357947</td>\n",
       "      <td>0.564614</td>\n",
       "      <td>0.356690</td>\n",
       "      <td>0.516447</td>\n",
       "      <td>0.281090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525082</td>\n",
       "      <td>0.364021</td>\n",
       "      <td>0.397494</td>\n",
       "      <td>0.625735</td>\n",
       "      <td>0.398518</td>\n",
       "      <td>0.518513</td>\n",
       "      <td>0.691217</td>\n",
       "      <td>0.548188</td>\n",
       "      <td>0.591037</td>\n",
       "      <td>0.309806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182744</td>\n",
       "      <td>0.607227</td>\n",
       "      <td>0.372086</td>\n",
       "      <td>0.142317</td>\n",
       "      <td>0.312902</td>\n",
       "      <td>0.222970</td>\n",
       "      <td>0.381215</td>\n",
       "      <td>0.356881</td>\n",
       "      <td>0.508104</td>\n",
       "      <td>0.517964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584386</td>\n",
       "      <td>0.362341</td>\n",
       "      <td>0.365578</td>\n",
       "      <td>0.533239</td>\n",
       "      <td>0.527541</td>\n",
       "      <td>0.315695</td>\n",
       "      <td>0.402996</td>\n",
       "      <td>0.240314</td>\n",
       "      <td>0.584384</td>\n",
       "      <td>0.467808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7295</th>\n",
       "      <td>0.072904</td>\n",
       "      <td>0.468162</td>\n",
       "      <td>0.303437</td>\n",
       "      <td>0.179915</td>\n",
       "      <td>0.339427</td>\n",
       "      <td>0.242976</td>\n",
       "      <td>0.398793</td>\n",
       "      <td>0.431053</td>\n",
       "      <td>0.477403</td>\n",
       "      <td>0.505311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432562</td>\n",
       "      <td>0.496237</td>\n",
       "      <td>0.446999</td>\n",
       "      <td>0.373658</td>\n",
       "      <td>0.368777</td>\n",
       "      <td>0.473945</td>\n",
       "      <td>0.361683</td>\n",
       "      <td>0.402358</td>\n",
       "      <td>0.381482</td>\n",
       "      <td>0.452864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7296</th>\n",
       "      <td>0.079883</td>\n",
       "      <td>0.367988</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>0.067432</td>\n",
       "      <td>0.287424</td>\n",
       "      <td>0.377541</td>\n",
       "      <td>0.359909</td>\n",
       "      <td>0.606959</td>\n",
       "      <td>0.414057</td>\n",
       "      <td>0.465056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456669</td>\n",
       "      <td>0.477203</td>\n",
       "      <td>0.497778</td>\n",
       "      <td>0.404697</td>\n",
       "      <td>0.416410</td>\n",
       "      <td>0.514228</td>\n",
       "      <td>0.444225</td>\n",
       "      <td>0.433322</td>\n",
       "      <td>0.457061</td>\n",
       "      <td>0.453789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7297</th>\n",
       "      <td>0.376556</td>\n",
       "      <td>0.468011</td>\n",
       "      <td>0.176175</td>\n",
       "      <td>0.241189</td>\n",
       "      <td>0.170486</td>\n",
       "      <td>0.206861</td>\n",
       "      <td>0.219945</td>\n",
       "      <td>0.312411</td>\n",
       "      <td>0.539920</td>\n",
       "      <td>0.459476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552605</td>\n",
       "      <td>0.426820</td>\n",
       "      <td>0.562936</td>\n",
       "      <td>0.265658</td>\n",
       "      <td>0.315330</td>\n",
       "      <td>0.492413</td>\n",
       "      <td>0.323093</td>\n",
       "      <td>0.432997</td>\n",
       "      <td>0.556258</td>\n",
       "      <td>0.295114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7298</th>\n",
       "      <td>0.240883</td>\n",
       "      <td>0.464419</td>\n",
       "      <td>0.216821</td>\n",
       "      <td>0.114380</td>\n",
       "      <td>0.159722</td>\n",
       "      <td>0.212885</td>\n",
       "      <td>0.243177</td>\n",
       "      <td>0.354506</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.465228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483761</td>\n",
       "      <td>0.469369</td>\n",
       "      <td>0.461921</td>\n",
       "      <td>0.518260</td>\n",
       "      <td>0.433740</td>\n",
       "      <td>0.497381</td>\n",
       "      <td>0.419791</td>\n",
       "      <td>0.469992</td>\n",
       "      <td>0.536159</td>\n",
       "      <td>0.415418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7299</th>\n",
       "      <td>0.037216</td>\n",
       "      <td>0.394271</td>\n",
       "      <td>0.268819</td>\n",
       "      <td>0.022301</td>\n",
       "      <td>0.370991</td>\n",
       "      <td>0.331862</td>\n",
       "      <td>0.433379</td>\n",
       "      <td>0.660561</td>\n",
       "      <td>0.364949</td>\n",
       "      <td>0.476931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475266</td>\n",
       "      <td>0.530897</td>\n",
       "      <td>0.385403</td>\n",
       "      <td>0.534720</td>\n",
       "      <td>0.372161</td>\n",
       "      <td>0.468533</td>\n",
       "      <td>0.417052</td>\n",
       "      <td>0.450995</td>\n",
       "      <td>0.498852</td>\n",
       "      <td>0.448002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7300 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0     0.116150  0.515373  0.306004  0.136208  0.346313  0.233494  0.351407   \n",
       "1     0.299883  0.607502  0.400980  0.303032  0.174298  0.198789  0.316660   \n",
       "2     0.146457  0.561705  0.391059  0.139404  0.312283  0.201222  0.210588   \n",
       "3     0.267506  0.741793  0.572944  0.408130  0.253209  0.357947  0.564614   \n",
       "4     0.182744  0.607227  0.372086  0.142317  0.312902  0.222970  0.381215   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7295  0.072904  0.468162  0.303437  0.179915  0.339427  0.242976  0.398793   \n",
       "7296  0.079883  0.367988  0.243277  0.067432  0.287424  0.377541  0.359909   \n",
       "7297  0.376556  0.468011  0.176175  0.241189  0.170486  0.206861  0.219945   \n",
       "7298  0.240883  0.464419  0.216821  0.114380  0.159722  0.212885  0.243177   \n",
       "7299  0.037216  0.394271  0.268819  0.022301  0.370991  0.331862  0.433379   \n",
       "\n",
       "          pca8      pca9     pca10  ...    pca231    pca232    pca233  \\\n",
       "0     0.464060  0.454801  0.500294  ...  0.396519  0.480992  0.386971   \n",
       "1     0.579717  0.432169  0.484432  ...  0.506064  0.589093  0.380978   \n",
       "2     0.518542  0.469579  0.554351  ...  0.465853  0.481245  0.401573   \n",
       "3     0.356690  0.516447  0.281090  ...  0.525082  0.364021  0.397494   \n",
       "4     0.356881  0.508104  0.517964  ...  0.584386  0.362341  0.365578   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "7295  0.431053  0.477403  0.505311  ...  0.432562  0.496237  0.446999   \n",
       "7296  0.606959  0.414057  0.465056  ...  0.456669  0.477203  0.497778   \n",
       "7297  0.312411  0.539920  0.459476  ...  0.552605  0.426820  0.562936   \n",
       "7298  0.354506  0.635400  0.465228  ...  0.483761  0.469369  0.461921   \n",
       "7299  0.660561  0.364949  0.476931  ...  0.475266  0.530897  0.385403   \n",
       "\n",
       "        pca234    pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "0     0.349232  0.390425  0.407799  0.210918  0.473298  0.386899  0.397036  \n",
       "1     0.481628  0.564485  0.757200  0.508659  0.636983  0.457745  0.582192  \n",
       "2     0.415580  0.313577  0.399847  0.370752  0.310011  0.481002  0.379264  \n",
       "3     0.625735  0.398518  0.518513  0.691217  0.548188  0.591037  0.309806  \n",
       "4     0.533239  0.527541  0.315695  0.402996  0.240314  0.584384  0.467808  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "7295  0.373658  0.368777  0.473945  0.361683  0.402358  0.381482  0.452864  \n",
       "7296  0.404697  0.416410  0.514228  0.444225  0.433322  0.457061  0.453789  \n",
       "7297  0.265658  0.315330  0.492413  0.323093  0.432997  0.556258  0.295114  \n",
       "7298  0.518260  0.433740  0.497381  0.419791  0.469992  0.536159  0.415418  \n",
       "7299  0.534720  0.372161  0.468533  0.417052  0.450995  0.498852  0.448002  \n",
       "\n",
       "[7300 rows x 240 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b652f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131700</td>\n",
       "      <td>0.561640</td>\n",
       "      <td>0.363172</td>\n",
       "      <td>0.114284</td>\n",
       "      <td>0.332871</td>\n",
       "      <td>0.301772</td>\n",
       "      <td>0.388933</td>\n",
       "      <td>0.358501</td>\n",
       "      <td>0.517014</td>\n",
       "      <td>0.367804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591854</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.402600</td>\n",
       "      <td>0.479817</td>\n",
       "      <td>0.429419</td>\n",
       "      <td>0.464436</td>\n",
       "      <td>0.662985</td>\n",
       "      <td>0.268016</td>\n",
       "      <td>0.615559</td>\n",
       "      <td>0.536855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.203354</td>\n",
       "      <td>0.563255</td>\n",
       "      <td>0.079622</td>\n",
       "      <td>0.766009</td>\n",
       "      <td>0.216915</td>\n",
       "      <td>0.230639</td>\n",
       "      <td>0.387035</td>\n",
       "      <td>0.432865</td>\n",
       "      <td>0.496112</td>\n",
       "      <td>0.342954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.541305</td>\n",
       "      <td>0.478727</td>\n",
       "      <td>0.407582</td>\n",
       "      <td>0.487967</td>\n",
       "      <td>0.246098</td>\n",
       "      <td>0.685748</td>\n",
       "      <td>0.623924</td>\n",
       "      <td>0.407216</td>\n",
       "      <td>0.186692</td>\n",
       "      <td>0.477419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375330</td>\n",
       "      <td>0.546294</td>\n",
       "      <td>0.233961</td>\n",
       "      <td>0.247737</td>\n",
       "      <td>0.231344</td>\n",
       "      <td>0.309722</td>\n",
       "      <td>0.228073</td>\n",
       "      <td>0.384507</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>0.413909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416262</td>\n",
       "      <td>0.545056</td>\n",
       "      <td>0.456798</td>\n",
       "      <td>0.547369</td>\n",
       "      <td>0.343920</td>\n",
       "      <td>0.589772</td>\n",
       "      <td>0.611621</td>\n",
       "      <td>0.481860</td>\n",
       "      <td>0.269983</td>\n",
       "      <td>0.427936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.173147</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>0.133736</td>\n",
       "      <td>0.361481</td>\n",
       "      <td>0.240259</td>\n",
       "      <td>0.223312</td>\n",
       "      <td>0.339781</td>\n",
       "      <td>0.582038</td>\n",
       "      <td>0.389090</td>\n",
       "      <td>0.376287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610497</td>\n",
       "      <td>0.415694</td>\n",
       "      <td>0.527106</td>\n",
       "      <td>0.385962</td>\n",
       "      <td>0.389033</td>\n",
       "      <td>0.571847</td>\n",
       "      <td>0.399187</td>\n",
       "      <td>0.447715</td>\n",
       "      <td>0.469906</td>\n",
       "      <td>0.369381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.327507</td>\n",
       "      <td>0.357077</td>\n",
       "      <td>0.162228</td>\n",
       "      <td>0.136617</td>\n",
       "      <td>0.237230</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.292412</td>\n",
       "      <td>0.417407</td>\n",
       "      <td>0.421941</td>\n",
       "      <td>0.429084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.610926</td>\n",
       "      <td>0.521451</td>\n",
       "      <td>0.486674</td>\n",
       "      <td>0.457482</td>\n",
       "      <td>0.408775</td>\n",
       "      <td>0.659278</td>\n",
       "      <td>0.551955</td>\n",
       "      <td>0.511312</td>\n",
       "      <td>0.499330</td>\n",
       "      <td>0.520611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.214661</td>\n",
       "      <td>0.469839</td>\n",
       "      <td>0.213835</td>\n",
       "      <td>0.191298</td>\n",
       "      <td>0.240104</td>\n",
       "      <td>0.254082</td>\n",
       "      <td>0.316210</td>\n",
       "      <td>0.451512</td>\n",
       "      <td>0.495531</td>\n",
       "      <td>0.384201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509453</td>\n",
       "      <td>0.411145</td>\n",
       "      <td>0.438763</td>\n",
       "      <td>0.311533</td>\n",
       "      <td>0.266806</td>\n",
       "      <td>0.531150</td>\n",
       "      <td>0.465450</td>\n",
       "      <td>0.485185</td>\n",
       "      <td>0.323974</td>\n",
       "      <td>0.481581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.290353</td>\n",
       "      <td>0.531186</td>\n",
       "      <td>0.278476</td>\n",
       "      <td>0.251553</td>\n",
       "      <td>0.211399</td>\n",
       "      <td>0.243714</td>\n",
       "      <td>0.339322</td>\n",
       "      <td>0.323989</td>\n",
       "      <td>0.497748</td>\n",
       "      <td>0.424467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482505</td>\n",
       "      <td>0.353654</td>\n",
       "      <td>0.424654</td>\n",
       "      <td>0.292479</td>\n",
       "      <td>0.461799</td>\n",
       "      <td>0.652800</td>\n",
       "      <td>0.224437</td>\n",
       "      <td>0.465236</td>\n",
       "      <td>0.528335</td>\n",
       "      <td>0.542585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.618315</td>\n",
       "      <td>0.395253</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>0.130415</td>\n",
       "      <td>0.154245</td>\n",
       "      <td>0.122545</td>\n",
       "      <td>0.701524</td>\n",
       "      <td>0.413694</td>\n",
       "      <td>0.436406</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369682</td>\n",
       "      <td>0.398361</td>\n",
       "      <td>0.483974</td>\n",
       "      <td>0.258372</td>\n",
       "      <td>0.440886</td>\n",
       "      <td>0.330597</td>\n",
       "      <td>0.520465</td>\n",
       "      <td>0.475259</td>\n",
       "      <td>0.396729</td>\n",
       "      <td>0.446762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.182194</td>\n",
       "      <td>0.377163</td>\n",
       "      <td>0.220923</td>\n",
       "      <td>0.132267</td>\n",
       "      <td>0.320563</td>\n",
       "      <td>0.291464</td>\n",
       "      <td>0.391263</td>\n",
       "      <td>0.578013</td>\n",
       "      <td>0.482401</td>\n",
       "      <td>0.467620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424180</td>\n",
       "      <td>0.514908</td>\n",
       "      <td>0.435538</td>\n",
       "      <td>0.387963</td>\n",
       "      <td>0.324912</td>\n",
       "      <td>0.350428</td>\n",
       "      <td>0.497980</td>\n",
       "      <td>0.293629</td>\n",
       "      <td>0.395315</td>\n",
       "      <td>0.433018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.191706</td>\n",
       "      <td>0.331949</td>\n",
       "      <td>0.351118</td>\n",
       "      <td>0.214566</td>\n",
       "      <td>0.311423</td>\n",
       "      <td>0.272306</td>\n",
       "      <td>0.510845</td>\n",
       "      <td>0.394661</td>\n",
       "      <td>0.515989</td>\n",
       "      <td>0.593310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456771</td>\n",
       "      <td>0.641255</td>\n",
       "      <td>0.426570</td>\n",
       "      <td>0.423269</td>\n",
       "      <td>0.434356</td>\n",
       "      <td>0.613405</td>\n",
       "      <td>0.389136</td>\n",
       "      <td>0.479236</td>\n",
       "      <td>0.368241</td>\n",
       "      <td>0.389705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0    0.131700  0.561640  0.363172  0.114284  0.332871  0.301772  0.388933   \n",
       "1    0.203354  0.563255  0.079622  0.766009  0.216915  0.230639  0.387035   \n",
       "2    0.375330  0.546294  0.233961  0.247737  0.231344  0.309722  0.228073   \n",
       "3    0.173147  0.425918  0.133736  0.361481  0.240259  0.223312  0.339781   \n",
       "4    0.327507  0.357077  0.162228  0.136617  0.237230  0.417356  0.292412   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.214661  0.469839  0.213835  0.191298  0.240104  0.254082  0.316210   \n",
       "996  0.290353  0.531186  0.278476  0.251553  0.211399  0.243714  0.339322   \n",
       "997  0.429850  0.618315  0.395253  0.158476  0.130415  0.154245  0.122545   \n",
       "998  0.182194  0.377163  0.220923  0.132267  0.320563  0.291464  0.391263   \n",
       "999  0.191706  0.331949  0.351118  0.214566  0.311423  0.272306  0.510845   \n",
       "\n",
       "         pca8      pca9     pca10  ...    pca231    pca232    pca233  \\\n",
       "0    0.358501  0.517014  0.367804  ...  0.591854  0.592040  0.402600   \n",
       "1    0.432865  0.496112  0.342954  ...  0.541305  0.478727  0.407582   \n",
       "2    0.384507  0.469000  0.413909  ...  0.416262  0.545056  0.456798   \n",
       "3    0.582038  0.389090  0.376287  ...  0.610497  0.415694  0.527106   \n",
       "4    0.417407  0.421941  0.429084  ...  0.610926  0.521451  0.486674   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.451512  0.495531  0.384201  ...  0.509453  0.411145  0.438763   \n",
       "996  0.323989  0.497748  0.424467  ...  0.482505  0.353654  0.424654   \n",
       "997  0.701524  0.413694  0.436406  ...  0.369682  0.398361  0.483974   \n",
       "998  0.578013  0.482401  0.467620  ...  0.424180  0.514908  0.435538   \n",
       "999  0.394661  0.515989  0.593310  ...  0.456771  0.641255  0.426570   \n",
       "\n",
       "       pca234    pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "0    0.479817  0.429419  0.464436  0.662985  0.268016  0.615559  0.536855  \n",
       "1    0.487967  0.246098  0.685748  0.623924  0.407216  0.186692  0.477419  \n",
       "2    0.547369  0.343920  0.589772  0.611621  0.481860  0.269983  0.427936  \n",
       "3    0.385962  0.389033  0.571847  0.399187  0.447715  0.469906  0.369381  \n",
       "4    0.457482  0.408775  0.659278  0.551955  0.511312  0.499330  0.520611  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.311533  0.266806  0.531150  0.465450  0.485185  0.323974  0.481581  \n",
       "996  0.292479  0.461799  0.652800  0.224437  0.465236  0.528335  0.542585  \n",
       "997  0.258372  0.440886  0.330597  0.520465  0.475259  0.396729  0.446762  \n",
       "998  0.387963  0.324912  0.350428  0.497980  0.293629  0.395315  0.433018  \n",
       "999  0.423269  0.434356  0.613405  0.389136  0.479236  0.368241  0.389705  \n",
       "\n",
       "[1000 rows x 240 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2336122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca1</th>\n",
       "      <th>pca2</th>\n",
       "      <th>pca3</th>\n",
       "      <th>pca4</th>\n",
       "      <th>pca5</th>\n",
       "      <th>pca6</th>\n",
       "      <th>pca7</th>\n",
       "      <th>pca8</th>\n",
       "      <th>pca9</th>\n",
       "      <th>pca10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca231</th>\n",
       "      <th>pca232</th>\n",
       "      <th>pca233</th>\n",
       "      <th>pca234</th>\n",
       "      <th>pca235</th>\n",
       "      <th>pca236</th>\n",
       "      <th>pca237</th>\n",
       "      <th>pca238</th>\n",
       "      <th>pca239</th>\n",
       "      <th>pca240</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.396136</td>\n",
       "      <td>0.474792</td>\n",
       "      <td>0.301072</td>\n",
       "      <td>0.365265</td>\n",
       "      <td>0.313068</td>\n",
       "      <td>0.322521</td>\n",
       "      <td>0.496225</td>\n",
       "      <td>0.231055</td>\n",
       "      <td>0.512063</td>\n",
       "      <td>0.497181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369724</td>\n",
       "      <td>0.389413</td>\n",
       "      <td>0.490386</td>\n",
       "      <td>0.417780</td>\n",
       "      <td>0.466116</td>\n",
       "      <td>0.488165</td>\n",
       "      <td>0.674575</td>\n",
       "      <td>0.302873</td>\n",
       "      <td>0.492148</td>\n",
       "      <td>0.618999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.327547</td>\n",
       "      <td>0.393308</td>\n",
       "      <td>0.145728</td>\n",
       "      <td>0.141959</td>\n",
       "      <td>0.310660</td>\n",
       "      <td>0.297931</td>\n",
       "      <td>0.368403</td>\n",
       "      <td>0.382150</td>\n",
       "      <td>0.378673</td>\n",
       "      <td>0.516289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527870</td>\n",
       "      <td>0.543647</td>\n",
       "      <td>0.329699</td>\n",
       "      <td>0.437378</td>\n",
       "      <td>0.312010</td>\n",
       "      <td>0.406945</td>\n",
       "      <td>0.398912</td>\n",
       "      <td>0.383071</td>\n",
       "      <td>0.533753</td>\n",
       "      <td>0.358153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.413367</td>\n",
       "      <td>0.485890</td>\n",
       "      <td>0.150088</td>\n",
       "      <td>0.145815</td>\n",
       "      <td>0.289947</td>\n",
       "      <td>0.170130</td>\n",
       "      <td>0.144081</td>\n",
       "      <td>0.555122</td>\n",
       "      <td>0.343464</td>\n",
       "      <td>0.495433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429950</td>\n",
       "      <td>0.507205</td>\n",
       "      <td>0.473251</td>\n",
       "      <td>0.288410</td>\n",
       "      <td>0.450655</td>\n",
       "      <td>0.592269</td>\n",
       "      <td>0.476355</td>\n",
       "      <td>0.293522</td>\n",
       "      <td>0.577681</td>\n",
       "      <td>0.432159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.186425</td>\n",
       "      <td>0.591963</td>\n",
       "      <td>0.090422</td>\n",
       "      <td>0.771335</td>\n",
       "      <td>0.273101</td>\n",
       "      <td>0.191065</td>\n",
       "      <td>0.204310</td>\n",
       "      <td>0.481354</td>\n",
       "      <td>0.379630</td>\n",
       "      <td>0.452354</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475807</td>\n",
       "      <td>0.313728</td>\n",
       "      <td>0.489172</td>\n",
       "      <td>0.384427</td>\n",
       "      <td>0.493645</td>\n",
       "      <td>0.679568</td>\n",
       "      <td>0.556721</td>\n",
       "      <td>0.464050</td>\n",
       "      <td>0.547089</td>\n",
       "      <td>0.547301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.466619</td>\n",
       "      <td>0.444289</td>\n",
       "      <td>0.070141</td>\n",
       "      <td>0.164449</td>\n",
       "      <td>0.338243</td>\n",
       "      <td>0.215287</td>\n",
       "      <td>0.273761</td>\n",
       "      <td>0.437498</td>\n",
       "      <td>0.377575</td>\n",
       "      <td>0.456425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.594441</td>\n",
       "      <td>0.305781</td>\n",
       "      <td>0.518325</td>\n",
       "      <td>0.403504</td>\n",
       "      <td>0.388609</td>\n",
       "      <td>0.673543</td>\n",
       "      <td>0.458846</td>\n",
       "      <td>0.554530</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.331690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>0.202830</td>\n",
       "      <td>0.664929</td>\n",
       "      <td>0.493121</td>\n",
       "      <td>0.135629</td>\n",
       "      <td>0.201343</td>\n",
       "      <td>0.202522</td>\n",
       "      <td>0.402809</td>\n",
       "      <td>0.666824</td>\n",
       "      <td>0.492569</td>\n",
       "      <td>0.592728</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635390</td>\n",
       "      <td>0.429808</td>\n",
       "      <td>0.480856</td>\n",
       "      <td>0.482727</td>\n",
       "      <td>0.378579</td>\n",
       "      <td>0.485992</td>\n",
       "      <td>0.289684</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.543936</td>\n",
       "      <td>0.385223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>0.625795</td>\n",
       "      <td>0.450687</td>\n",
       "      <td>0.145132</td>\n",
       "      <td>0.314103</td>\n",
       "      <td>0.558202</td>\n",
       "      <td>0.265878</td>\n",
       "      <td>0.317725</td>\n",
       "      <td>0.628775</td>\n",
       "      <td>0.300960</td>\n",
       "      <td>0.549484</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335243</td>\n",
       "      <td>0.440740</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.626936</td>\n",
       "      <td>0.308017</td>\n",
       "      <td>0.387023</td>\n",
       "      <td>0.454425</td>\n",
       "      <td>0.466321</td>\n",
       "      <td>0.669567</td>\n",
       "      <td>0.514217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>0.117567</td>\n",
       "      <td>0.567959</td>\n",
       "      <td>0.409485</td>\n",
       "      <td>0.120271</td>\n",
       "      <td>0.273163</td>\n",
       "      <td>0.224111</td>\n",
       "      <td>0.348736</td>\n",
       "      <td>0.570673</td>\n",
       "      <td>0.506556</td>\n",
       "      <td>0.426725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444499</td>\n",
       "      <td>0.496470</td>\n",
       "      <td>0.499937</td>\n",
       "      <td>0.483574</td>\n",
       "      <td>0.438781</td>\n",
       "      <td>0.426838</td>\n",
       "      <td>0.384147</td>\n",
       "      <td>0.544530</td>\n",
       "      <td>0.537683</td>\n",
       "      <td>0.452222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>0.085886</td>\n",
       "      <td>0.466995</td>\n",
       "      <td>0.296659</td>\n",
       "      <td>0.093542</td>\n",
       "      <td>0.345641</td>\n",
       "      <td>0.289391</td>\n",
       "      <td>0.413669</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.451750</td>\n",
       "      <td>0.491584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637670</td>\n",
       "      <td>0.553132</td>\n",
       "      <td>0.515340</td>\n",
       "      <td>0.267160</td>\n",
       "      <td>0.434753</td>\n",
       "      <td>0.430305</td>\n",
       "      <td>0.385092</td>\n",
       "      <td>0.488598</td>\n",
       "      <td>0.275365</td>\n",
       "      <td>0.480217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>0.595281</td>\n",
       "      <td>0.424381</td>\n",
       "      <td>0.142692</td>\n",
       "      <td>0.344685</td>\n",
       "      <td>0.556010</td>\n",
       "      <td>0.343471</td>\n",
       "      <td>0.576904</td>\n",
       "      <td>0.575407</td>\n",
       "      <td>0.401690</td>\n",
       "      <td>0.679515</td>\n",
       "      <td>...</td>\n",
       "      <td>0.618269</td>\n",
       "      <td>0.228439</td>\n",
       "      <td>0.342018</td>\n",
       "      <td>0.342493</td>\n",
       "      <td>0.100709</td>\n",
       "      <td>0.498606</td>\n",
       "      <td>0.560382</td>\n",
       "      <td>0.661874</td>\n",
       "      <td>0.563478</td>\n",
       "      <td>0.658409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3500 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca1      pca2      pca3      pca4      pca5      pca6      pca7  \\\n",
       "0     0.396136  0.474792  0.301072  0.365265  0.313068  0.322521  0.496225   \n",
       "1     0.327547  0.393308  0.145728  0.141959  0.310660  0.297931  0.368403   \n",
       "2     0.413367  0.485890  0.150088  0.145815  0.289947  0.170130  0.144081   \n",
       "3     0.186425  0.591963  0.090422  0.771335  0.273101  0.191065  0.204310   \n",
       "4     0.466619  0.444289  0.070141  0.164449  0.338243  0.215287  0.273761   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "3495  0.202830  0.664929  0.493121  0.135629  0.201343  0.202522  0.402809   \n",
       "3496  0.625795  0.450687  0.145132  0.314103  0.558202  0.265878  0.317725   \n",
       "3497  0.117567  0.567959  0.409485  0.120271  0.273163  0.224111  0.348736   \n",
       "3498  0.085886  0.466995  0.296659  0.093542  0.345641  0.289391  0.413669   \n",
       "3499  0.595281  0.424381  0.142692  0.344685  0.556010  0.343471  0.576904   \n",
       "\n",
       "          pca8      pca9     pca10  ...    pca231    pca232    pca233  \\\n",
       "0     0.231055  0.512063  0.497181  ...  0.369724  0.389413  0.490386   \n",
       "1     0.382150  0.378673  0.516289  ...  0.527870  0.543647  0.329699   \n",
       "2     0.555122  0.343464  0.495433  ...  0.429950  0.507205  0.473251   \n",
       "3     0.481354  0.379630  0.452354  ...  0.475807  0.313728  0.489172   \n",
       "4     0.437498  0.377575  0.456425  ...  0.594441  0.305781  0.518325   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "3495  0.666824  0.492569  0.592728  ...  0.635390  0.429808  0.480856   \n",
       "3496  0.628775  0.300960  0.549484  ...  0.335243  0.440740  0.462687   \n",
       "3497  0.570673  0.506556  0.426725  ...  0.444499  0.496470  0.499937   \n",
       "3498  0.510870  0.451750  0.491584  ...  0.637670  0.553132  0.515340   \n",
       "3499  0.575407  0.401690  0.679515  ...  0.618269  0.228439  0.342018   \n",
       "\n",
       "        pca234    pca235    pca236    pca237    pca238    pca239    pca240  \n",
       "0     0.417780  0.466116  0.488165  0.674575  0.302873  0.492148  0.618999  \n",
       "1     0.437378  0.312010  0.406945  0.398912  0.383071  0.533753  0.358153  \n",
       "2     0.288410  0.450655  0.592269  0.476355  0.293522  0.577681  0.432159  \n",
       "3     0.384427  0.493645  0.679568  0.556721  0.464050  0.547089  0.547301  \n",
       "4     0.403504  0.388609  0.673543  0.458846  0.554530  0.404290  0.331690  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "3495  0.482727  0.378579  0.485992  0.289684  0.350263  0.543936  0.385223  \n",
       "3496  0.626936  0.308017  0.387023  0.454425  0.466321  0.669567  0.514217  \n",
       "3497  0.483574  0.438781  0.426838  0.384147  0.544530  0.537683  0.452222  \n",
       "3498  0.267160  0.434753  0.430305  0.385092  0.488598  0.275365  0.480217  \n",
       "3499  0.342493  0.100709  0.498606  0.560382  0.661874  0.563478  0.658409  \n",
       "\n",
       "[3500 rows x 240 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d853b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a610773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.55\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95       913\n",
      "           1       0.49      0.49      0.49        87\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.72      0.72      0.72      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n",
      "0.4942528735632184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mLPC=MLPClassifier(max_iter=30000)\n",
    "mLPC.fit(X_train_, Y_train_over)\n",
    "y_pred = mLPC.predict(X_test_)\n",
    "y_pred_raw=mLPC.predict(X_test2)\n",
    "print(round(mLPC.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_mLPC=f1_score(y_test,y_pred)\n",
    "print(f1_mLPC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54cd9b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       913\n",
      "           1       0.35      0.62      0.45        87\n",
      "\n",
      "    accuracy                           0.87      1000\n",
      "   macro avg       0.66      0.76      0.69      1000\n",
      "weighted avg       0.91      0.87      0.88      1000\n",
      "\n",
      "0.45000000000000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1523: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 12.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lg_df=LogisticRegression(C=10,max_iter=10000,n_jobs=-1,penalty='l2',solver='liblinear')\n",
    "lg_df.fit(X_train_, Y_train_over)\n",
    "y_pred = lg_df.predict(X_test_)\n",
    "y_pred_raw=lg_df.predict(X_test2)\n",
    "print(round(lg_df.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_log=f1_score(y_test,y_pred)\n",
    "print(f1_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d296d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.03\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       913\n",
      "           1       0.63      0.51      0.56        87\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.79      0.74      0.76      1000\n",
      "weighted avg       0.93      0.93      0.93      1000\n",
      "\n",
      "0.5605095541401274\n"
     ]
    }
   ],
   "source": [
    "svc = SVC(kernel='rbf',C = 0.7, gamma = 0.5, probability=True)\n",
    "svc.fit(X_train_, Y_train_over)\n",
    "y_pred = svc.predict(X_test_)\n",
    "y_pred_raw=svc.predict(X_test2)\n",
    "print(round(svc.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_svc=f1_score(y_test,y_pred)\n",
    "print(f1_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4075de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       913\n",
      "           1       0.52      0.62      0.57        87\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.74      0.78      0.76      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n",
      "0.5654450261780105\n"
     ]
    }
   ],
   "source": [
    "svc2 = SVC(kernel='rbf',C = 0.5, gamma = 0.3,probability=True)\n",
    "svc2.fit(X_train_, Y_train_over)\n",
    "y_pred = svc2.predict(X_test_)\n",
    "y_pred_raw=svc2.predict(X_test2)\n",
    "print(round(svc2.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_svc2=f1_score(y_test,y_pred)\n",
    "print(f1_svc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ff6537cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       913\n",
      "           1       0.34      0.57      0.43        87\n",
      "\n",
      "    accuracy                           0.86      1000\n",
      "   macro avg       0.65      0.73      0.67      1000\n",
      "weighted avg       0.90      0.86      0.88      1000\n",
      "\n",
      "0.42553191489361697\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 2,weights='distance',n_jobs=-1)\n",
    "knn.fit(X_train_, Y_train_over)\n",
    "y_pred = knn.predict(X_test_)\n",
    "y_pred_raw=knn.predict(X_test2)\n",
    "print(round(knn.score(X_train_, Y_train_over)* 100, 2))#knn에는 overfitting이 너무 심했나?\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_knn=f1_score(y_test,y_pred)\n",
    "print(f1_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5038fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.82\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       913\n",
      "           1       0.43      0.47      0.45        87\n",
      "\n",
      "    accuracy                           0.90      1000\n",
      "   macro avg       0.69      0.71      0.70      1000\n",
      "weighted avg       0.90      0.90      0.90      1000\n",
      "\n",
      "0.4505494505494505\n"
     ]
    }
   ],
   "source": [
    "poly_svc =SVC(kernel='poly',degree=20,C = 0.4, gamma = 0.2,max_iter=1000,probability=True)\n",
    "poly_svc.fit(X_train_, Y_train_over)\n",
    "y_pred = poly_svc.predict(X_test_)\n",
    "y_pred_raw=poly_svc.predict(X_test2)\n",
    "print(round(poly_svc.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_poly_svc=f1_score(y_test,y_pred)\n",
    "print(f1_poly_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9a74871e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96       913\n",
      "           1       0.53      0.52      0.52        87\n",
      "\n",
      "    accuracy                           0.92      1000\n",
      "   macro avg       0.74      0.74      0.74      1000\n",
      "weighted avg       0.92      0.92      0.92      1000\n",
      "\n",
      "0.5232558139534884\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(loss='log', max_iter=100000,random_state=100) #loss='hinge'는 linear와 같다.\n",
    "sgd.fit(X_train_, Y_train_over)\n",
    "y_pred = sgd.predict(X_test_)\n",
    "y_pred_raw=sgd.predict(X_test2)\n",
    "print(round(sgd.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_sgd=f1_score(y_test,y_pred)\n",
    "print(f1_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5a7c9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.04\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82       913\n",
      "           1       0.17      0.60      0.26        87\n",
      "\n",
      "    accuracy                           0.71      1000\n",
      "   macro avg       0.56      0.66      0.54      1000\n",
      "weighted avg       0.88      0.71      0.77      1000\n",
      "\n",
      "0.2646310432569975\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth=3,min_samples_split=4)\n",
    "decision_tree.fit(X_train_, Y_train_over)\n",
    "y_pred = decision_tree.predict(X_test_)\n",
    "y_pred_raw=decision_tree.predict(X_test2)\n",
    "print(round(decision_tree.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_decision_tree=f1_score(y_test,y_pred)\n",
    "print(f1_decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e46b0e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.45\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.90       913\n",
      "           1       0.25      0.48      0.33        87\n",
      "\n",
      "    accuracy                           0.83      1000\n",
      "   macro avg       0.60      0.67      0.62      1000\n",
      "weighted avg       0.89      0.83      0.85      1000\n",
      "\n",
      "0.3307086614173228\n"
     ]
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=1000,max_depth=300,min_samples_split=5,n_jobs=-1,max_leaf_nodes=5)\n",
    "random_forest.fit(X_train_, Y_train_over)\n",
    "y_pred = random_forest.predict(X_test_)\n",
    "y_pred_raw=random_forest.predict(X_test2)\n",
    "print(round(random_forest.score(X_train_, Y_train_over)* 100, 2))#랜덤포레스트는 신이 아니였나?\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_random_forest=f1_score(y_test,y_pred)\n",
    "print(f1_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eda58e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.08\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.65      0.77       913\n",
      "           1       0.13      0.54      0.21        87\n",
      "\n",
      "    accuracy                           0.64      1000\n",
      "   macro avg       0.53      0.59      0.49      1000\n",
      "weighted avg       0.87      0.64      0.72      1000\n",
      "\n",
      "0.20659340659340664\n"
     ]
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train_, Y_train_over)\n",
    "y_pred = gaussian.predict(X_test_)\n",
    "y_pred_raw=gaussian.predict(X_test2)\n",
    "print(round(gaussian.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_gaussian=f1_score(y_test,y_pred)\n",
    "print(f1_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67d1edcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.82      0.88       913\n",
      "           1       0.26      0.69      0.38        87\n",
      "\n",
      "    accuracy                           0.81      1000\n",
      "   macro avg       0.61      0.75      0.63      1000\n",
      "weighted avg       0.90      0.81      0.84      1000\n",
      "\n",
      "0.380952380952381\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron(max_iter=100, eta0=0.2)\n",
    "perceptron.fit(X_train_, Y_train_over)\n",
    "y_pred = perceptron.predict(X_test_)\n",
    "y_pred_raw=perceptron.predict(X_test2)\n",
    "print(round(perceptron.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))#상당히 낮게 나온다\n",
    "f1_perceptron=f1_score(y_test,y_pred)\n",
    "print(f1_perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b976857c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>0.560510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.450549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.435798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.425532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.380952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.264631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.206593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model     Score\n",
       "0     Support Vector Machines  0.560510\n",
       "7                  Linear SVC  0.450549\n",
       "2         Logistic Regression  0.435798\n",
       "1                         KNN  0.425532\n",
       "6  Stochastic Gradient Decent  0.421053\n",
       "5                  Perceptron  0.380952\n",
       "3               Random Forest  0.343750\n",
       "8               Decision Tree  0.264631\n",
       "4                 Naive Bayes  0.206593"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [f1_svc, f1_knn, f1_log, \n",
    "              f1_random_forest, f1_gaussian, f1_perceptron, \n",
    "              f1_sgd, f1_poly_svc, f1_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27be4aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       913\n",
      "           1       0.62      0.45      0.52        87\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.78      0.71      0.74      1000\n",
      "weighted avg       0.92      0.93      0.92      1000\n",
      "\n",
      "0.52\n"
     ]
    }
   ],
   "source": [
    "lgbm=LGBMClassifier(n_estimators=1000,max_depth=4,num_leaves=10,n_jobs=1,application='binary',objective='binary')\n",
    "lgbm.fit(X_train_, Y_train_over)\n",
    "y_pred = lgbm.predict(X_test_)\n",
    "y_pred_raw=lgbm.predict(X_test2)\n",
    "print(round(perceptron.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))#상당히 낮게 나온다\n",
    "f1_lgbm=f1_score(y_test,y_pred)\n",
    "print(f1_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6438caa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:10:50] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:10:50] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "89.05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95       913\n",
      "           1       0.46      0.51      0.48        87\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.71      0.72      0.72      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n",
      "0.4835164835164835\n"
     ]
    }
   ],
   "source": [
    "Xgb=XGBClassifier(objective='binary:logistic',colsample_bytree= 0.8,learning_rate= .1,max_depth= 5,min_child_weight=2,reg_lambda=10,reg_alpha=10,gamma=0.2,n_estimator=1000)\n",
    "Xgb.fit(X_train_, Y_train_over)\n",
    "y_pred = Xgb.predict(X_test_)\n",
    "y_pred_raw=Xgb.predict(X_test2)\n",
    "print(round(perceptron.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_Xgb=f1_score(y_test,y_pred)\n",
    "print(f1_Xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e7bf209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.12\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91       913\n",
      "           1       0.32      0.67      0.44        87\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.64      0.77      0.67      1000\n",
      "weighted avg       0.91      0.85      0.87      1000\n",
      "\n",
      "0.4360902255639098\n"
     ]
    }
   ],
   "source": [
    "ridge=RidgeClassifier()\n",
    "ridge.fit(X_train_, Y_train_over)\n",
    "y_pred = ridge.predict(X_test_)\n",
    "y_pred_raw=ridge.predict(X_test2)\n",
    "print(round(ridge.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_ridge=f1_score(y_test,y_pred)\n",
    "print(f1_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95750f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      1141\n",
      "           1       0.26      0.42      0.32       109\n",
      "\n",
      "    accuracy                           0.84      1250\n",
      "   macro avg       0.60      0.65      0.62      1250\n",
      "weighted avg       0.88      0.84      0.86      1250\n",
      "\n",
      "0.32055749128919864\n"
     ]
    }
   ],
   "source": [
    "ada=AdaBoostClassifier()\n",
    "ada.fit(X_train_, Y_train_over)\n",
    "y_pred = ada.predict(X_test_)\n",
    "y_pred_raw=ada.predict(X_test2)\n",
    "print(round(ada.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_ada=f1_score(y_test,y_pred)\n",
    "print(f1_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "68af4e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.96      1141\n",
      "           1       0.67      0.28      0.40       109\n",
      "\n",
      "    accuracy                           0.93      1250\n",
      "   macro avg       0.80      0.64      0.68      1250\n",
      "weighted avg       0.91      0.93      0.91      1250\n",
      "\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "gbc=GradientBoostingClassifier(max_depth=4,min_samples_leaf=10,n_estimators=1000)\n",
    "gbc.fit(X_train_, Y_train_over)\n",
    "y_pred = gbc.predict(X_test_)\n",
    "y_pred_raw=gbc.predict(X_test2)\n",
    "print(round(gbc.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_gbc=f1_score(y_test,y_pred)\n",
    "print(f1_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5c8afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('svm', svc),\n",
    "    ('poly',poly_svc)\n",
    "    ,('Xgb',Xgb)\n",
    "    ,('knn', knn)\n",
    "    ,('lgbm',lgbm) \n",
    "    ,('sgd', sgd)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2b523",
   "metadata": {},
   "outputs": [],
   "source": [
    ",('knn', knn) ,('Xgb',Xgb)   ('sgd', sgd),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eafc8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "models2 = [\n",
    "    ('svm', svc),\n",
    "    ('poly',poly_svc)\n",
    "    ,('Xgb',Xgb)\n",
    "    ,('knn', knn)\n",
    "    ,('lgbm',lgbm) \n",
    "    ,('sgd', sgd)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7be3528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:18:51] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:18:51] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "models3 = [\n",
    "    ('sgd1', sgd),\n",
    "    ('svm', svc)\n",
    "    ,('Xgb',Xgb)\n",
    "    ,('lgbm',lgbm)\n",
    "]\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote3  = VotingClassifier(models3, voting='soft')\n",
    "soft_vote3.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote3.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "38aaa8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:27:21] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:27:21] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5517241379310345\n"
     ]
    }
   ],
   "source": [
    "models5 = [\n",
    "    ('svm', svc),\n",
    "    ('sgd1', sgd),\n",
    "    ('Xgb',Xgb)\n",
    "    ,('lgbm',lgbm)\n",
    "]\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote5  = VotingClassifier(models5, voting='soft')\n",
    "soft_vote5.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote5.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "793c0b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:28:38] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:28:38] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:00] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:29:00] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:19] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:29:19] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:29:37] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:29:37] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "models6 = [\n",
    "    ('sv1', soft_vote),\n",
    "    ('sv2', soft_vote2),\n",
    "    ('sv3', soft_vote3),\n",
    "    ('sv5', soft_vote5)\n",
    "]\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote6  = VotingClassifier(models6, voting='soft')\n",
    "soft_vote6.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote6.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1fd4b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:26] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:34:26] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:48] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:34:48] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:35:10] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:35:10] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5827814569536425\n"
     ]
    }
   ],
   "source": [
    "models4 = [\n",
    "    ('sv1', soft_vote),\n",
    "    ('sv2', soft_vote2),\n",
    "    ('sv3', soft_vote3)\n",
    "]\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote4  = VotingClassifier(models4, voting='soft')\n",
    "soft_vote4.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote4.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b4e5f4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       913\n",
      "           1       0.69      0.51      0.58        87\n",
      "\n",
      "    accuracy                           0.94      1000\n",
      "   macro avg       0.82      0.74      0.77      1000\n",
      "weighted avg       0.93      0.94      0.93      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c0a8d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:10:27] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:10:27] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.5714285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote2  = VotingClassifier(models2, voting='soft')\n",
    "soft_vote2.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote2.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c1193af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.49      0.65       913\n",
      "           1       0.13      0.82      0.23        87\n",
      "\n",
      "    accuracy                           0.52      1000\n",
      "   macro avg       0.55      0.65      0.44      1000\n",
      "weighted avg       0.89      0.52      0.62      1000\n",
      "\n",
      "0.2286634460547504\n"
     ]
    }
   ],
   "source": [
    "linear_svc =SVC(kernel='linear',C = 0.2, gamma = 0.1,max_iter=1000,probability=True)\n",
    "linear_svc.fit(X_train_, Y_train_over)\n",
    "y_pred = linear_svc.predict(X_test_)\n",
    "y_pred_raw=linear_svc.predict(X_test2)\n",
    "print(round(linear_svc.score(X_train_, Y_train_over)* 100, 2))\n",
    "print(classification_report(y_test,y_pred))\n",
    "f1_linear_svc=f1_score(y_test,y_pred)\n",
    "print(f1_linear_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1bb07fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\2469l\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:12:07] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimator\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[17:12:07] WARNING: ..\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.567741935483871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "soft_vote  = VotingClassifier(models, voting='soft')\n",
    "soft_vote.fit(X_train_, Y_train_over)\n",
    "prediction=soft_vote.predict(X_test_)\n",
    "print(f1_score(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dfd45692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       913\n",
      "           1       0.65      0.51      0.57        87\n",
      "\n",
      "    accuracy                           0.93      1000\n",
      "   macro avg       0.80      0.74      0.77      1000\n",
      "weighted avg       0.93      0.93      0.93      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3a171364",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction3=soft_vote.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fb447d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type : <class 'numpy.ndarray'> | Length : 3500\n"
     ]
    }
   ],
   "source": [
    "print(f'Type : {type(prediction3)} | Length : {len(prediction3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e79d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction3_df=pd.DataFrame(prediction3,columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "410cfd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0         3264\n",
       "1          236\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction3_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffb8739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('path.csv', prediction3,fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a938aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type : <class 'numpy.ndarray'> | Length : 3500\n"
     ]
    }
   ],
   "source": [
    "prediction4=soft_vote4.predict(X_test2)\n",
    "print(f'Type : {type(prediction4)} | Length : {len(prediction4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "583a4205",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('path2.csv', prediction3,fmt='%d', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d73865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction4_df=pd.DataFrame(prediction4,columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c6f6322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0         3273\n",
       "1          227\n",
       "dtype: int64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction4_df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2005086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.DataFrame(prediction3==prediction4, columns=['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "3c4a371a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value\n",
       "True     3475\n",
       "False      25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
